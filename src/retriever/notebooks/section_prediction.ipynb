{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akBumB45ABYa",
        "outputId": "cc35d592-f3f5-4e97-de87-6a903e8e597a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dvS3v9W3m7t",
        "outputId": "0ecd3264-18b6-4daa-f4b4-63b491843447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/multidoc-conv-qa/src/retriever\n"
          ]
        }
      ],
      "source": [
        "!git config --global user.name \"alistvt\"\n",
        "!git config --global user.email \"alistvt@gmail.com\"\n",
        "\n",
        "%cd /content/drive/MyDrive/multidoc-conv-qa/src/retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HCHw5gxO0Jx-"
      },
      "outputs": [],
      "source": [
        "!pip install -r ../../requirements.txt --quiet\n",
        "!pip install --quiet transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKBvBzEAGLtH"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "opgDLO5tGJxI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.functional import normalize\n",
        "from tqdm import tqdm\n",
        "from datasets import load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5akM0VXmzYku"
      },
      "source": [
        "# Section prediction\n",
        "\n",
        "In this notebook I will train a sequence classifier to have a model which gets a question and a section and defines whether the section is related to the question or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9vpLyUkzWzO"
      },
      "source": [
        "# Dataset\n",
        "### Dataset Description\n",
        "\n",
        "- **mutldoc2dial_doc.json** contains the documents that are indexed by key `domain` and `doc_id` . Each document instance includes the following,\n",
        "\n",
        "  - `doc_id`: the ID of a document;\n",
        "  - `title`: the title of the document;\n",
        "  - `domain`: the domain of the document;\n",
        "  - `doc_text`: the text content of the document (without HTML markups);\n",
        "  - `doc_html_ts`: the document content with HTML markups and the annotated spans that are indicated by `text_id` attribute, which corresponds to `id_sp`.\n",
        "  - `doc_html_raw`: the document content with HTML markups and without span annotations.\n",
        "  - `spans`: key-value pairs of all spans in the document, with `id_sp` as key. Each span includes the following,\n",
        "    - `id_sp`: the id of a  span as noted by `text_id` in  `doc_html_ts`;\n",
        "    - `start_sp`/  `end_sp`: the start/end position of the text span in `doc_text`;\n",
        "    - `text_sp`: the text content of the span.\n",
        "    - `id_sec`: the id of the (sub)section (e.g. `<p>`) or title (`<h2>`) that contains the span.\n",
        "    - `start_sec` / `end_sec`: the start/end position of the (sub)section in `doc_text`.\n",
        "    - `text_sec`: the text of the (sub)section.\n",
        "    - `title`: the title of the (sub)section.\n",
        "    - `parent_titles`: the parent titles of the `title`.\n",
        "\n",
        "- **multidoc2dial_dial_train.json** and **multidoc2dial_dial_validation.json**  contain the training and dev split of dialogue data that are indexed by key `domain` . Please note: **For test split, we only include a dummy file in this version.**\n",
        "\n",
        "  Each dialogue instance includes the following,\n",
        "\n",
        "  - `dial_id`: the ID of a dialogue;\n",
        "  - `turns`: a list of dialogue turns. Each turn includes,\n",
        "    - `turn_id`: the time order of the turn;\n",
        "    - `role`: either \"agent\" or \"user\";READ\n",
        "    - `da`: dialogue act;\n",
        "    - `references`: a list of spans with `id_sp` ,  `label` and `doc_id`. `references` is empty if a turn is for indicating previous user query not answerable or irrelevant to the document. **Note** that labels \"*precondition*\"/\"*solution*\" are fuzzy annotations that indicate whether a span is for describing a conditional context or a solution.\n",
        "    - `utterance`: the human-generated utterance based on the dialogue scene.\n",
        "Downloading the training dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVN8XSp13LjU"
      },
      "source": [
        "## Constructing the Section prediction Dataset\n",
        "\n",
        "``` question | section_title | section_text | label```\n",
        "\n",
        "label: show whether the question's answer is inside the section_text or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0wp6Mw7Q3LjV"
      },
      "outputs": [],
      "source": [
        "def construct_sections_dict(filepath):\n",
        "    \"sections[domain][doc_id][sec_id]['text'/'title'/'spans: list']\"\n",
        "    sections = {}\n",
        "    import json\n",
        "    with open(filepath, 'r') as f:\n",
        "        multidoc2dial_docs = json.load(f)['doc_data']\n",
        "    \n",
        "    for domain in multidoc2dial_docs:\n",
        "        sections[domain] = {}\n",
        "\n",
        "    for domain, domain_docs in multidoc2dial_docs.items():\n",
        "        for doc_id in domain_docs:\n",
        "            sections[domain][doc_id] = {}\n",
        "    \n",
        "    for domain, domain_docs in multidoc2dial_docs.items():\n",
        "        for doc_id, doc_data in domain_docs.items():\n",
        "            for span_id, span_data in doc_data['spans'].items():\n",
        "                id_sec = span_data['id_sec']\n",
        "                text_sec = span_data['text_sec'].strip()\n",
        "                title_sec = span_data['title'].strip()\n",
        "                if id_sec not in sections[domain][doc_id]:\n",
        "                    sections[domain][doc_id][id_sec] = {\n",
        "                        'title': title_sec,\n",
        "                        'text': text_sec,\n",
        "                        'spans': [span_id]\n",
        "                    }\n",
        "                else:\n",
        "                    sections[domain][doc_id][id_sec]['spans'].append(span_id)\n",
        "\n",
        "    return sections    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TsNXGEdnEn4r"
      },
      "outputs": [],
      "source": [
        "sections_dict = construct_sections_dict('../../dataset/multidoc2dial/v1.0/multidoc2dial_doc.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ApUaULDHFzxT"
      },
      "outputs": [],
      "source": [
        "def construct_sections_dataset(filepath, sections_dict):\n",
        "    \"sections_dataset[question | section_title | section_text | label]\"\n",
        "    sections_dataset = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        questions_dataset = json.load(f)['dial_data']\n",
        "    \n",
        "    for domain, domain_dials in questions_dataset.items():\n",
        "        for dial in domain_dials:\n",
        "            for i, turn in enumerate(dial['turns'][:-1]):\n",
        "                if turn['role'] == 'user':\n",
        "                    if dial['turns'][i+1]['role'] == 'agent':\n",
        "                        agent_turn = dial['turns'][i+1]\n",
        "                        question = dial['turns'][i]['utterance']\n",
        "                        if len(agent_turn['references']):\n",
        "                            reference = agent_turn['references'][0]\n",
        "                            doc_id = reference['doc_id']\n",
        "                            span_id = reference['id_sp']\n",
        "\n",
        "                            for section_id, section in sections_dict[domain][doc_id].items():\n",
        "                                if span_id in section['spans']:\n",
        "                                    label = True\n",
        "                                else:\n",
        "                                    label = False\n",
        "                                sections_dataset.append({\n",
        "                                    'question': question,\n",
        "                                    'section_title': section['title'],\n",
        "                                    'section_text': section['text'],\n",
        "                                    'label': label\n",
        "                                })\n",
        "                        else:\n",
        "                            # TODO: unknown\n",
        "                            pass\n",
        "                    else:\n",
        "                        continue\n",
        "    return sections_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nPSTMsgAI-R_"
      },
      "outputs": [],
      "source": [
        "train_dataset = construct_sections_dataset('../../dataset/multidoc2dial/v1.0/multidoc2dial_dial_train.json', sections_dict)\n",
        "test_dataset = construct_sections_dataset('../../dataset/multidoc2dial/v1.0/multidoc2dial_dial_validation.json', sections_dict)\n",
        "\n",
        "train_df = pd.DataFrame(train_dataset)\n",
        "test_df = pd.DataFrame(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjFpNyxiK3Gm",
        "outputId": "01ed0920-8011-4307-81e9-add1b60a3da2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    610072\n",
              "True      22806\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNPG-waE3LjX",
        "outputId": "ab900e3b-19a6-4835-e8f9-d554c8797700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16.038759274942326\n",
            "27.50204918032787\n"
          ]
        }
      ],
      "source": [
        "lens = [len(sections_dict[domain][doc_id]) for domain, domain_data in sections_dict.items() for doc_id in domain_data.keys() ]\n",
        "print(np.std(lens))\n",
        "print(np.mean(lens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "vYbgJuaS3LjZ",
        "outputId": "17842ad9-f93f-46c2-c5e8-22d45beb7f14"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1ebfe2f8-dcca-4a15-be23-07276289cc05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>section_title</th>\n",
              "      <th>section_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hello, I forgot o update my address, can you h...</td>\n",
              "      <td></td>\n",
              "      <td>Many DMV customers make easily avoidable mista...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hello, I forgot o update my address, can you h...</td>\n",
              "      <td>1. Forgetting to Update Address</td>\n",
              "      <td>1. Forgetting to Update Address</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hello, I forgot o update my address, can you h...</td>\n",
              "      <td>1. Forgetting to Update Address</td>\n",
              "      <td>By statute , you must report a change of addre...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello, I forgot o update my address, can you h...</td>\n",
              "      <td>1. Forgetting to Update Address</td>\n",
              "      <td>It is not sufficient to only: write your new a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello, I forgot o update my address, can you h...</td>\n",
              "      <td>1. Forgetting to Update Address</td>\n",
              "      <td>Learn more about how to change the address on ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632873</th>\n",
              "      <td>Can you forgive the student loan?</td>\n",
              "      <td>I ve missed one or more loan payments.</td>\n",
              "      <td>I ve missed one or more loan payments.</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632874</th>\n",
              "      <td>Can you forgive the student loan?</td>\n",
              "      <td>I ve missed one or more loan payments.</td>\n",
              "      <td>Stay in touch with your loan servicer especial...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632875</th>\n",
              "      <td>Can you forgive the student loan?</td>\n",
              "      <td>I ve missed one or more loan payments.</td>\n",
              "      <td>One thing you definitely want to avoid is goin...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632876</th>\n",
              "      <td>Can you forgive the student loan?</td>\n",
              "      <td>Having Your Student Loan Forgiven</td>\n",
              "      <td>Having Your Student Loan Forgiven</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632877</th>\n",
              "      <td>Can you forgive the student loan?</td>\n",
              "      <td>Having Your Student Loan Forgiven</td>\n",
              "      <td>You are generally required to repay your stude...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>632878 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ebfe2f8-dcca-4a15-be23-07276289cc05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ebfe2f8-dcca-4a15-be23-07276289cc05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ebfe2f8-dcca-4a15-be23-07276289cc05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                 question  \\\n",
              "0       Hello, I forgot o update my address, can you h...   \n",
              "1       Hello, I forgot o update my address, can you h...   \n",
              "2       Hello, I forgot o update my address, can you h...   \n",
              "3       Hello, I forgot o update my address, can you h...   \n",
              "4       Hello, I forgot o update my address, can you h...   \n",
              "...                                                   ...   \n",
              "632873                  Can you forgive the student loan?   \n",
              "632874                  Can you forgive the student loan?   \n",
              "632875                  Can you forgive the student loan?   \n",
              "632876                  Can you forgive the student loan?   \n",
              "632877                  Can you forgive the student loan?   \n",
              "\n",
              "                                 section_title  \\\n",
              "0                                                \n",
              "1              1. Forgetting to Update Address   \n",
              "2              1. Forgetting to Update Address   \n",
              "3              1. Forgetting to Update Address   \n",
              "4              1. Forgetting to Update Address   \n",
              "...                                        ...   \n",
              "632873  I ve missed one or more loan payments.   \n",
              "632874  I ve missed one or more loan payments.   \n",
              "632875  I ve missed one or more loan payments.   \n",
              "632876       Having Your Student Loan Forgiven   \n",
              "632877       Having Your Student Loan Forgiven   \n",
              "\n",
              "                                             section_text  label  \n",
              "0       Many DMV customers make easily avoidable mista...  False  \n",
              "1                         1. Forgetting to Update Address  False  \n",
              "2       By statute , you must report a change of addre...   True  \n",
              "3       It is not sufficient to only: write your new a...  False  \n",
              "4       Learn more about how to change the address on ...  False  \n",
              "...                                                   ...    ...  \n",
              "632873             I ve missed one or more loan payments.  False  \n",
              "632874  Stay in touch with your loan servicer especial...  False  \n",
              "632875  One thing you definitely want to avoid is goin...  False  \n",
              "632876                  Having Your Student Loan Forgiven  False  \n",
              "632877  You are generally required to repay your stude...   True  \n",
              "\n",
              "[632878 rows x 4 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJjHIp3PRFGG"
      },
      "source": [
        "## Model and tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-321y6SsRETj"
      },
      "outputs": [],
      "source": [
        "model_name = \"setu4993/LaBSE\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gF0ziqxJ3Lja"
      },
      "outputs": [],
      "source": [
        "def combine_sample(sample, sep_token=\" <SEP> \"):\n",
        "    return sample['question'] + sep_token + sample['section_title'] + sep_token + sample['section_text']\n",
        "\n",
        "\n",
        "def tokenize_function(examples, prediction=False, cuda=False):\n",
        "    if prediction:\n",
        "        tokenized = tokenizer(examples['combined'], max_length=512, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
        "    else:\n",
        "        tokenized = tokenizer(examples['combined'], max_length=512, padding=\"max_length\", truncation=True)\n",
        "    if cuda:\n",
        "        tokenized_cuda = {}\n",
        "        for key, value in tokenized.items():\n",
        "            tokenized_cuda[key] = value.cuda()\n",
        "        return tokenized_cuda\n",
        "    else:\n",
        "        return tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3IbMkVd3Lja"
      },
      "source": [
        "### constructing dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "722a7de74690427e9a2538a6965a6e04",
            "7991b551ea8d4e2a947f9f0f750e96d2",
            "fc536b7ff46e4081b1762d33a1637532",
            "b953a54e64a74078bedd3aae52f0e258",
            "c22ca34e00364120a7b2180b9518e6ed",
            "e6f7cb84494d4491b28ed179a2e62711",
            "6787942a897f4169a067a72fd3974541",
            "443a4cdd89194c3696ac5cb18b6a9328",
            "3923065cc1534667bd96088e69709aca",
            "3cdc1c3e278c4c37990a9c7cfce1421e",
            "a263e7081ca44bb1bb48273a940f386e",
            "4d65d5afc12c49ceb046ea73687388cb",
            "14562548596c44e599ca871259b5ea6e",
            "cff01ce8b1f94d8a8d152d91d4ab7010",
            "add5004177c1484eb8e07a85ddf05b08",
            "c2578573080b45ef81cd825729746cda",
            "3d1460f426b8489fb479f16ab0e12866",
            "24d413de422a43228d22fe615c7666d6",
            "ea9fcf6a5a044df69c4c28afa41dc75a",
            "2b74ed454e0745ed89ad9371a5a65336",
            "09335cf1773b4a85a0473975a37d0bce",
            "5a529bbb865f4cc6835de492b0465b6e"
          ]
        },
        "id": "n89j9i8A3Ljb",
        "outputId": "db9c5fca-ae8c-4859-dcac-254351dad254"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7fa3fc897a70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "722a7de74690427e9a2538a6965a6e04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/633 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d65d5afc12c49ceb046ea73687388cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/125 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "train_df['combined'] = train_df.apply(combine_sample, axis = 1)\n",
        "test_df['combined'] = test_df.apply(combine_sample, axis = 1)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "tokenized_trainset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_testset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "secpred_dataset = DatasetDict()\n",
        "\n",
        "secpred_dataset['train'] = tokenized_trainset\n",
        "secpred_dataset['validation'] = tokenized_testset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtOJcbeo5rBQ"
      },
      "source": [
        "# Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr3Qbox83Ljc"
      },
      "source": [
        "## FCN based on [cls]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zRT_9kl3Ljd"
      },
      "source": [
        "### AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oFuQ6Tq8eoC"
      },
      "source": [
        "We use a classification method on the questions to decide the relationship between the previous question and current question.\n",
        "In the dataset provided to us, previous turn documents are predefined, meaning that we are aware of the previous documents, therefore, if the prediction predicts that current question's document is the same as the previous, we don't need to retrieve a document and we give the previous doc_id."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsHE7D873Ljd"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRkLfGS43Lje",
        "outputId": "838ab834-7dde-4074-b104-cae7e02d2418"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at setu4993/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(501153, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"setu4993/LaBSE\"\n",
        "\n",
        "secpred_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "secpred_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcI4P0Lt3Lje"
      },
      "source": [
        "#### metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HgLwYYuQ3Ljf"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDy2n4oi3Ljg"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G-eckBPu3Ljg"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/home/',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy ='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    # auto_find_batch_size=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=secpred_model,\n",
        "    args=training_args,\n",
        "    train_dataset=secpred_dataset['train'],\n",
        "    eval_dataset=secpred_dataset['validation'],\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "aev9XuyUA0DA",
        "outputId": "a7d78991-d466-4bde-b64c-088b7eedbc67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, combined, section_title, section_text. If question, combined, section_title, section_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 632878\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 949317\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='465' max='949317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   465/949317 03:04 < 104:54:03, 2.51 it/s, Epoch 0.00/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1160' max='949317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  1160/949317 07:44 < 105:34:22, 2.49 it/s, Epoch 0.00/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m                         \u001b[0moptimizer_was_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_before\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mscale_after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0moptimizer_was_run\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOIe5Tn643ZO"
      },
      "outputs": [],
      "source": [
        "def predict_FUDNet_DR_TEIT(data, k=1):\n",
        "    inputs = tokenize_function(data, prediction=True, cuda=True)\n",
        "    outputs = fudnet_model(**inputs)\n",
        "    is_followup = bool(torch.argmax(outputs.logits))\n",
        "    \n",
        "    if is_followup:\n",
        "        dr_scores, dr_predictions = predict_DR_TEIT([data['prev_answer'], data['question'], data['history']], k=k)\n",
        "        return dr_predictions\n",
        "    else:\n",
        "        dr_scores, dr_predictions = predict_DR_TEIT([data['question']], k=k)\n",
        "        return dr_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YCZ4JOhRoyQ",
        "outputId": "8b3eb94d-f39b-4961-9808-d4c82ba01908"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "history                         Don't do that I'll get insurance\n",
              "question       I have, that is why I am here to clear that up...\n",
              "combined       Don't do that I'll get insurance <SEP> I have,...\n",
              "followup                                                       1\n",
              "prev_doc            Top 5 DMV Mistakes and How to Avoid Them#3_0\n",
              "current_doc         Top 5 DMV Mistakes and How to Avoid Them#3_0\n",
              "prev_answer    Okay, have you received a letter from the DMV ...\n",
              "Name: 2, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.loc[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlqEPT-y_Jtb",
        "outputId": "e54d3192-67a9-435e-a5a1-9d133df12bc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Top 5 DMV Mistakes and How to Avoid Them#3_0',\n",
              " 'Respond to DMV insurance letters and orders#3_0',\n",
              " 'How to change your address#1_0',\n",
              " 'Insurance lapses#3_0',\n",
              " 'Information about transaction entries#3_0']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_FUDNet_DR_TEIT(test_df.loc[2], k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkPVjtFr1YFH"
      },
      "source": [
        "#### Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OWpfm4n1X0m"
      },
      "outputs": [],
      "source": [
        "def get_nearests_doc_ids(text, k=None):\n",
        "    x_embed = get_embeddings(text).reshape(1, -1)\n",
        "    neighs = doc_predictor.kneighbors(x_embed)\n",
        "    return [doc_ids[x] for x in neighs[1][0,:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfJ4rjG135d"
      },
      "outputs": [],
      "source": [
        "def predict_KNN_FUDNet_DR_TEIT(queries, k=1, alpha=10):\n",
        "    \"\"\"\n",
        "    Predict which document is matched to the given query.\n",
        "\n",
        "    :param queries: input queries in time reversed order (latest first)\n",
        "    :type queries: str (or list of strs)\n",
        "    :param k: number of returning docs\n",
        "    :type k: int \n",
        "    :return: return the document names and accuracies\n",
        "    \"\"\"\n",
        "\n",
        "    idf_score = np.array(list(map(lambda x: 0.0, title_embeddings)))\n",
        "    tfidf_score = np.array(list(map(lambda x: 0.0, title_embeddings)))\n",
        "    coef_sum = 0\n",
        "    for i, query in enumerate(queries):\n",
        "        query_embd = get_embeddings(query)\n",
        "        query_sim = list(map(lambda x: np.dot(x, query_embd) /\n",
        "                            (np.linalg.norm(query_embd) * np.linalg.norm(x)),\n",
        "                            title_embeddings))\n",
        "        query_sim = np.array(query_sim)\n",
        "        # coef = 2**(-i) * calc_idf_score(query)\n",
        "        coef = calc_idf_score(query)\n",
        "        coef_sum += coef\n",
        "\n",
        "        idf_score += coef * query_sim\n",
        "        tfidf_score += coef * np.squeeze(np.asarray(tfidf_wm @ tfidfVectorizer.transform([query]).todense().T))\n",
        "\n",
        "    scores = (idf_score + alpha * tfidf_score) / coef_sum\n",
        "    best_k_idx = scores.argsort()[::-1][:k]\n",
        "    scores = scores[best_k_idx]\n",
        "    predictions = list(map(lambda x: titles[x], best_k_idx))\n",
        "    return (scores, predictions)\n",
        "\n",
        "def predict_KNN_FUDNet_DR_TEIT(data, k=1):\n",
        "    inputs = tokenize_function(data, prediction=True, cuda=True)\n",
        "    outputs = fudnet_model(**inputs)\n",
        "    is_followup = bool(torch.argmax(outputs.logits))\n",
        "    \n",
        "    if is_followup:\n",
        "        dr_scores, dr_predictions = predict_DR_TEIT([data['prev_answer'], data['question'], data['history']], k=k)\n",
        "        return dr_predictions\n",
        "    else:\n",
        "        dr_scores, dr_predictions = predict_DR_TEIT([data['question']], k=k)\n",
        "        return dr_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyjeHgyD1xRJ"
      },
      "source": [
        "#### Fit knn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esl3eIf010na",
        "outputId": "1771b428-7015-4570-deca-4bf8160937fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=100)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "doc_predictor = KNeighborsClassifier(n_neighbors=100)\n",
        "doc_predictor.fit(span_embeddings, doc_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td3uXbaV35lk",
        "outputId": "8f3590f8-a737-4b92-b791-5dd231be6e9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Top 5 DMV Mistakes and How to Avoid Them#3_0',\n",
              " 'Get a vehicle registration or title record (abstract)#1_0',\n",
              " 'In-transit vehicle permits (temporary registrations)#3_0',\n",
              " 'Exchange your out-of-state driver license#1_0',\n",
              " 'Appeal a TVB ticket conviction#1_0']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_nearests_doc_ids(\"5 DMV Mistakes\")[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H32jwXIw5KIw"
      },
      "source": [
        "# Test\n",
        "In the test dataset we just picked ones with **user** turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STHinF2c558c"
      },
      "outputs": [],
      "source": [
        "def test_loop(df, predictor):\n",
        "    prec_at_50 = 0\n",
        "    prec_at_10 = 0\n",
        "    prec_at_5 = 0\n",
        "    prec_at_1 = 0\n",
        "    ranks = []\n",
        "    for index, data in tqdm(df.iterrows()):\n",
        "        predictions = predictor(data, k=500)\n",
        "        actual_doc = data['current_doc']\n",
        "        ranks.append(1 / (predictions.index(actual_doc) + 1))\n",
        "        if actual_doc == predictions[0]:\n",
        "            prec_at_1 += 1\n",
        "        if actual_doc in predictions[:5]:\n",
        "            prec_at_5 += 1\n",
        "        if actual_doc in predictions[:10]:\n",
        "            prec_at_10 += 1\n",
        "        if actual_doc in predictions[:50]:\n",
        "            prec_at_50 += 1\n",
        "\n",
        "        if index % 100 == 99:\n",
        "            print(f\"\"\"\n",
        "                MRR: mean={np.array(ranks).mean()}, var={np.array(ranks).var()}\n",
        "                Prec@(1) = {prec_at_1 / index}\n",
        "                Prec@(5) = {prec_at_5 / index}\n",
        "                Prec@(10) = {prec_at_10 / index}\n",
        "                Prec@(50) = {prec_at_50 / index}\n",
        "                NUMBER_OF_SAMPLES = {index}\n",
        "            \"\"\")\n",
        "\n",
        "    return f\"\"\"\n",
        "        MRR: mean={np.array(ranks).mean()}, var={np.array(ranks).var()}\n",
        "        Prec@(1) = {prec_at_1 / index}\n",
        "        Prec@(5) = {prec_at_5 / index}\n",
        "        Prec@(10) = {prec_at_10 / index}\n",
        "        Prec@(50) = {prec_at_50 / index}\n",
        "        NUMBER_OF_SAMPLES = {index}\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kbiRFDJg2QP6",
        "outputId": "37d1a117-a0d9-456e-aa25-6bc27018cd8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:33,  3.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7039079215980597, var=0.14377394068348706\n",
            "                Prec@(1) = 0.6060606060606061\n",
            "                Prec@(5) = 0.8181818181818182\n",
            "                Prec@(10) = 0.9292929292929293\n",
            "                Prec@(50) = 1.0\n",
            "                NUMBER_OF_SAMPLES = 99\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200it [01:07,  3.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7902209363599058, var=0.11812314929333972\n",
            "                Prec@(1) = 0.7135678391959799\n",
            "                Prec@(5) = 0.8793969849246231\n",
            "                Prec@(10) = 0.9346733668341709\n",
            "                Prec@(50) = 0.9899497487437185\n",
            "                NUMBER_OF_SAMPLES = 199\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "300it [01:43,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7853334234094211, var=0.11404097012093398\n",
            "                Prec@(1) = 0.6923076923076923\n",
            "                Prec@(5) = 0.8929765886287625\n",
            "                Prec@(10) = 0.9364548494983278\n",
            "                Prec@(50) = 0.9866220735785953\n",
            "                NUMBER_OF_SAMPLES = 299\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400it [02:15,  3.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7850967764162591, var=0.11437552187926402\n",
            "                Prec@(1) = 0.6917293233082706\n",
            "                Prec@(5) = 0.899749373433584\n",
            "                Prec@(10) = 0.9348370927318296\n",
            "                Prec@(50) = 0.9799498746867168\n",
            "                NUMBER_OF_SAMPLES = 399\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "501it [02:58,  3.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7882768463264257, var=0.11015128150599376\n",
            "                Prec@(1) = 0.6893787575150301\n",
            "                Prec@(5) = 0.9078156312625251\n",
            "                Prec@(10) = 0.9398797595190381\n",
            "                Prec@(50) = 0.9819639278557114\n",
            "                NUMBER_OF_SAMPLES = 499\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "600it [03:36,  3.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7880566865477531, var=0.10964060689882264\n",
            "                Prec@(1) = 0.6878130217028381\n",
            "                Prec@(5) = 0.9081803005008348\n",
            "                Prec@(10) = 0.9382303839732888\n",
            "                Prec@(50) = 0.9833055091819699\n",
            "                NUMBER_OF_SAMPLES = 599\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "700it [04:29,  1.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7711605122925687, var=0.11688227500693\n",
            "                Prec@(1) = 0.6680972818311874\n",
            "                Prec@(5) = 0.8969957081545065\n",
            "                Prec@(10) = 0.9284692417739628\n",
            "                Prec@(50) = 0.9785407725321889\n",
            "                NUMBER_OF_SAMPLES = 699\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "801it [05:08,  3.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7625910081509559, var=0.12298149426928057\n",
            "                Prec@(1) = 0.6633291614518148\n",
            "                Prec@(5) = 0.8811013767209012\n",
            "                Prec@(10) = 0.9224030037546934\n",
            "                Prec@(50) = 0.9737171464330413\n",
            "                NUMBER_OF_SAMPLES = 799\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "900it [05:43,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7604726031570741, var=0.12537084161644643\n",
            "                Prec@(1) = 0.664071190211346\n",
            "                Prec@(5) = 0.8765294771968855\n",
            "                Prec@(10) = 0.917686318131257\n",
            "                Prec@(50) = 0.9699666295884316\n",
            "                NUMBER_OF_SAMPLES = 899\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1001it [06:32,  3.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7580197257129236, var=0.1270136792078571\n",
            "                Prec@(1) = 0.6626626626626627\n",
            "                Prec@(5) = 0.8728728728728729\n",
            "                Prec@(10) = 0.914914914914915\n",
            "                Prec@(50) = 0.96996996996997\n",
            "                NUMBER_OF_SAMPLES = 999\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1100it [07:07,  3.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7605851824924286, var=0.12850709866222307\n",
            "                Prec@(1) = 0.6706096451319381\n",
            "                Prec@(5) = 0.8698817106460418\n",
            "                Prec@(10) = 0.910828025477707\n",
            "                Prec@(50) = 0.9681528662420382\n",
            "                NUMBER_OF_SAMPLES = 1099\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1200it [07:47,  2.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7530367746197558, var=0.12989672150513235\n",
            "                Prec@(1) = 0.658882402001668\n",
            "                Prec@(5) = 0.8707256046705588\n",
            "                Prec@(10) = 0.9107589658048374\n",
            "                Prec@(50) = 0.9658048373644704\n",
            "                NUMBER_OF_SAMPLES = 1199\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1301it [08:28,  3.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7535198360823495, var=0.13173748424909867\n",
            "                Prec@(1) = 0.6628175519630485\n",
            "                Prec@(5) = 0.8668206312548113\n",
            "                Prec@(10) = 0.9045419553502695\n",
            "                Prec@(50) = 0.9599692070823711\n",
            "                NUMBER_OF_SAMPLES = 1299\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1400it [09:13,  2.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7391630560855046, var=0.13610311972580597\n",
            "                Prec@(1) = 0.6440314510364546\n",
            "                Prec@(5) = 0.860614724803431\n",
            "                Prec@(10) = 0.8970693352394568\n",
            "                Prec@(50) = 0.9571122230164403\n",
            "                NUMBER_OF_SAMPLES = 1399\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1500it [09:47,  2.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7269237547580063, var=0.13903621766414445\n",
            "                Prec@(1) = 0.627751834556371\n",
            "                Prec@(5) = 0.8545697131420947\n",
            "                Prec@(10) = 0.895930620413609\n",
            "                Prec@(50) = 0.9566377585056705\n",
            "                NUMBER_OF_SAMPLES = 1499\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1601it [10:21,  3.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7310236351716247, var=0.1373266616357569\n",
            "                Prec@(1) = 0.6316447779862414\n",
            "                Prec@(5) = 0.858036272670419\n",
            "                Prec@(10) = 0.8986866791744841\n",
            "                Prec@(50) = 0.9568480300187617\n",
            "                NUMBER_OF_SAMPLES = 1599\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1701it [10:52,  4.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7329194565754593, var=0.13822727921431974\n",
            "                Prec@(1) = 0.6368452030606239\n",
            "                Prec@(5) = 0.8557975279576221\n",
            "                Prec@(10) = 0.8964096527369041\n",
            "                Prec@(50) = 0.9540906415538553\n",
            "                NUMBER_OF_SAMPLES = 1699\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1800it [11:28,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7336799141229156, var=0.1378391221799688\n",
            "                Prec@(1) = 0.6375764313507504\n",
            "                Prec@(5) = 0.8565869927737632\n",
            "                Prec@(10) = 0.896609227348527\n",
            "                Prec@(50) = 0.9560867148415787\n",
            "                NUMBER_OF_SAMPLES = 1799\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1900it [11:59,  3.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7322733903396611, var=0.13851126855596751\n",
            "                Prec@(1) = 0.6361242759347024\n",
            "                Prec@(5) = 0.8530805687203792\n",
            "                Prec@(10) = 0.8957345971563981\n",
            "                Prec@(50) = 0.956292785676672\n",
            "                NUMBER_OF_SAMPLES = 1899\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2000it [12:49,  2.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7313164663563175, var=0.1383302847041666\n",
            "                Prec@(1) = 0.6338169084542271\n",
            "                Prec@(5) = 0.8529264632316158\n",
            "                Prec@(10) = 0.8954477238619309\n",
            "                Prec@(50) = 0.9569784892446224\n",
            "                NUMBER_OF_SAMPLES = 1999\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2101it [13:33,  3.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7271388522695879, var=0.13944357948275662\n",
            "                Prec@(1) = 0.6283944735588376\n",
            "                Prec@(5) = 0.8508813720819438\n",
            "                Prec@(10) = 0.894235350166746\n",
            "                Prec@(50) = 0.957122439256789\n",
            "                NUMBER_OF_SAMPLES = 2099\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2200it [14:05,  2.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.723542382947126, var=0.1406470447279826\n",
            "                Prec@(1) = 0.6243747157799\n",
            "                Prec@(5) = 0.8476580263756253\n",
            "                Prec@(10) = 0.891768985902683\n",
            "                Prec@(50) = 0.9558890404729422\n",
            "                NUMBER_OF_SAMPLES = 2199\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2300it [14:40,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.719230739821631, var=0.14126418155816367\n",
            "                Prec@(1) = 0.6180948238364506\n",
            "                Prec@(5) = 0.8473249238799478\n",
            "                Prec@(10) = 0.8916920400173989\n",
            "                Prec@(50) = 0.955632883862549\n",
            "                NUMBER_OF_SAMPLES = 2299\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2401it [15:23,  3.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7166043225585198, var=0.14138614416474332\n",
            "                Prec@(1) = 0.6131721550646102\n",
            "                Prec@(5) = 0.846602751146311\n",
            "                Prec@(10) = 0.8903709879116298\n",
            "                Prec@(50) = 0.9533138807836599\n",
            "                NUMBER_OF_SAMPLES = 2399\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2500it [16:00,  2.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.721063199419645, var=0.1405457320910995\n",
            "                Prec@(1) = 0.6194477791116446\n",
            "                Prec@(5) = 0.849139655862345\n",
            "                Prec@(10) = 0.8915566226490597\n",
            "                Prec@(50) = 0.9531812725090036\n",
            "                NUMBER_OF_SAMPLES = 2499\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2600it [16:43,  3.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.722510265749097, var=0.1403124225201101\n",
            "                Prec@(1) = 0.6213928434013082\n",
            "                Prec@(5) = 0.849942285494421\n",
            "                Prec@(10) = 0.8911119661408234\n",
            "                Prec@(50) = 0.9519045786841093\n",
            "                NUMBER_OF_SAMPLES = 2599\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2701it [17:17,  3.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7217094196183785, var=0.14003790054827087\n",
            "                Prec@(1) = 0.6194886995183402\n",
            "                Prec@(5) = 0.8506854390515005\n",
            "                Prec@(10) = 0.8929233049277511\n",
            "                Prec@(50) = 0.952204520192664\n",
            "                NUMBER_OF_SAMPLES = 2699\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2801it [17:54,  3.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7223807243507471, var=0.13969835180704113\n",
            "                Prec@(1) = 0.6202215076813148\n",
            "                Prec@(5) = 0.8520900321543409\n",
            "                Prec@(10) = 0.8942479456948911\n",
            "                Prec@(50) = 0.9524830296534477\n",
            "                NUMBER_OF_SAMPLES = 2799\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2900it [18:27,  3.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7268969361953773, var=0.13817245005125406\n",
            "                Prec@(1) = 0.6253880648499482\n",
            "                Prec@(5) = 0.8554674025526043\n",
            "                Prec@(10) = 0.8968609865470852\n",
            "                Prec@(50) = 0.953432218006209\n",
            "                NUMBER_OF_SAMPLES = 2899\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3000it [19:01,  3.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7259220563803565, var=0.13812352498361205\n",
            "                Prec@(1) = 0.6235411803934645\n",
            "                Prec@(5) = 0.855618539513171\n",
            "                Prec@(10) = 0.8972990996999\n",
            "                Prec@(50) = 0.9533177725908636\n",
            "                NUMBER_OF_SAMPLES = 2999\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3101it [19:35,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.721743178286175, var=0.13915418542513366\n",
            "                Prec@(1) = 0.6179412713778638\n",
            "                Prec@(5) = 0.8535011293965795\n",
            "                Prec@(10) = 0.8960955146821555\n",
            "                Prec@(50) = 0.9515972894482091\n",
            "                NUMBER_OF_SAMPLES = 3099\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3201it [20:18,  3.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7166168681972046, var=0.13989449025728767\n",
            "                Prec@(1) = 0.6105032822757112\n",
            "                Prec@(5) = 0.8533916849015317\n",
            "                Prec@(10) = 0.8962175679899969\n",
            "                Prec@(50) = 0.9518599562363238\n",
            "                NUMBER_OF_SAMPLES = 3199\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3300it [21:06,  2.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7180104455873062, var=0.13963758594989065\n",
            "                Prec@(1) = 0.6123067596241285\n",
            "                Prec@(5) = 0.8538951197332525\n",
            "                Prec@(10) = 0.89602909972719\n",
            "                Prec@(50) = 0.9511973325250076\n",
            "                NUMBER_OF_SAMPLES = 3299\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3400it [21:45,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7177283341566983, var=0.13979892037900063\n",
            "                Prec@(1) = 0.6119446896145925\n",
            "                Prec@(5) = 0.852603706972639\n",
            "                Prec@(10) = 0.8952633127390409\n",
            "                Prec@(50) = 0.950867902324213\n",
            "                NUMBER_OF_SAMPLES = 3399\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3501it [22:16,  3.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7138783496257615, var=0.14071535614341785\n",
            "                Prec@(1) = 0.606744784224064\n",
            "                Prec@(5) = 0.84938553872535\n",
            "                Prec@(10) = 0.8931123178050872\n",
            "                Prec@(50) = 0.9499857102029151\n",
            "                NUMBER_OF_SAMPLES = 3499\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3600it [22:46,  3.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7114744397200978, var=0.14055294094629675\n",
            "                Prec@(1) = 0.6021116976938038\n",
            "                Prec@(5) = 0.8499583217560434\n",
            "                Prec@(10) = 0.8935815504306752\n",
            "                Prec@(50) = 0.9499861072520145\n",
            "                NUMBER_OF_SAMPLES = 3599\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3700it [23:19,  2.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7094340380451104, var=0.14127969312336003\n",
            "                Prec@(1) = 0.6001622060016221\n",
            "                Prec@(5) = 0.8480670451473371\n",
            "                Prec@(10) = 0.8932143822654771\n",
            "                Prec@(50) = 0.9505271695052717\n",
            "                NUMBER_OF_SAMPLES = 3699\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3801it [23:52,  3.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.710894325342702, var=0.14116219797553214\n",
            "                Prec@(1) = 0.6025269807844169\n",
            "                Prec@(5) = 0.8481179257699395\n",
            "                Prec@(10) = 0.8933929981574098\n",
            "                Prec@(50) = 0.9507765201368781\n",
            "                NUMBER_OF_SAMPLES = 3799\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3900it [24:21,  4.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7058745122229784, var=0.1429643224396008\n",
            "                Prec@(1) = 0.5973326493972814\n",
            "                Prec@(5) = 0.8430366760707874\n",
            "                Prec@(10) = 0.8897153116183637\n",
            "                Prec@(50) = 0.9505001282380098\n",
            "                NUMBER_OF_SAMPLES = 3899\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4000it [24:52,  2.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                MRR: mean=0.7077374154308121, var=0.14241631380864428\n",
            "                Prec@(1) = 0.5993998499624906\n",
            "                Prec@(5) = 0.8447111777944486\n",
            "                Prec@(10) = 0.8904726181545386\n",
            "                Prec@(50) = 0.9504876219054764\n",
            "                NUMBER_OF_SAMPLES = 3999\n",
            "            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4002it [24:53,  2.68it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-f62abf028221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-89-77413af3fe88>\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_FUDNet_DR_TEIT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mactual_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_doc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-249f72cb4912>\u001b[0m in \u001b[0;36mpredict_FUDNet_DR_TEIT\u001b[0;34m(data, k)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_followup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdr_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_DR_TEIT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prev_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdr_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-4e20684b4bfc>\u001b[0m in \u001b[0;36mpredict_DR_TEIT\u001b[0;34m(queries, k, alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcoef_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mquery_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         query_sim = list(map(lambda x: np.dot(x, query_embd) /\n\u001b[1;32m     18\u001b[0m                             \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3eece1f5e79c>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(sentece)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                 padding=True)\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 )\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    516\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(test_loop(train_df, predict_FUDNet_DR_TEIT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdGwYLfIxV3X"
      },
      "outputs": [],
      "source": [
        "print(test_loop(train_df, predict_KNN_FUDNet_DR_TEIT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH9vqWah_hFc"
      },
      "source": [
        "# Results\n",
        "\n",
        "At last we have resutls as follows:\n",
        "\n",
        "\n",
        "| Method | @1 | @5 | @10 | @50 | @100 | MRR (mean, var) |\n",
        "|:------:|:------:|:------:|:-------:|:-------:|:--------:|:---:|\n",
        "| IDF - vanilla | 13% | 30% | 39% | 64% | 83% | (0.22, 0.11) |\n",
        "| IDF - power-order | 15% | 31% | 41% | 65% | 83% | (0.23, 0.12) |\n",
        "| IDF - power-order (softmax) | 10.7% | 23% | 31% | 57.6% | 78% | (0.18, 0.09) |\n",
        "| IDF - self-attention | 13.9% | 29% | 38% | 62% | 82% | (0.22, 0.11) |\n",
        "| DR. TEIT | 61.6% | 86% | 91% | 96% | 98% | (0.72, 0.13) |\n",
        "| DR. TEIT (val) | 40.0% | 69.25% | 92.4% | 99% | 98% | (0.53, 0.16) |\n",
        "| FUDNet + DR. TEIT | 67% | 87% | 91% | 96% | 99% | (0.76, 0.12) |\n",
        "| FUDNet + DR. TEIT (val) | 48.2% | 75.48% | 82% | 93% | 99% | (0.60, 0.16) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SczaywmN6Y0m"
      },
      "source": [
        "MRR: mean=0.7605851824924286, var=0.12850709866222307\n",
        "                Prec@(1) = 0.6706096451319381\n",
        "                Prec@(5) = 0.8698817106460418\n",
        "                Prec@(10) = 0.910828025477707\n",
        "                Prec@(50) = 0.9681528662420382\n",
        "                NUMBER_OF_SAMPLES = 1099"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ3M9TEuFq_M"
      },
      "source": [
        "# drafts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxPrLsnz-ccc"
      },
      "outputs": [],
      "source": [
        "def simple_fudnet_test():\n",
        "  data = {\"combined\": \" <SEP> Hello. how can you help me?\", \"label\": 0}\n",
        "  inputs = tokenize_function(data, prediction=True, cuda=True)\n",
        "  labels = torch.tensor([data['label']]).unsqueeze(0)  # Batch size 1\n",
        "  outputs = fudnet_model(**inputs)\n",
        "  prediction = torch.argmax(outputs.logits)\n",
        "  print(prediction)\n",
        "\n",
        "simple_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbPuutLoOvsK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5EncoderModel\n",
        "import torch\n",
        "\n",
        "model_name = [\"setu4993/LaBSE\", \"t5-small\", \"bert-base-uncased\"][0]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "# model = T5EncoderModel.from_pretrained(\"t5-base\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(\"inputs\", inputs)\n",
        "\n",
        "print(\"last_hidden_states\", last_hidden_states.shape)\n",
        "\n",
        "# pooler = outputs.pooler_output\n",
        "# print(\"pooler\",pooler.shape)\n",
        "# with torch.no_grad():\n",
        "#     print(np.squeeze(np.array(pooler)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlFbxcxK3Ljs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from utils import scaled_dot_product_attention\n",
        "\n",
        "inputs = tokenizer([\"Hello, my dog is cute\", \"Yes this is a beautiful dog and you can have it.\"], max_length=16, padding=\"max_length\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "query=last_hidden_states[0, :, :]\n",
        "key=last_hidden_states[1, :, :]\n",
        "value=last_hidden_states[1, :, :]\n",
        "\n",
        "context, attention = scaled_dot_product_attention(\n",
        "    query=query,\n",
        "    key=key,\n",
        "    value=value,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQVqMhWX3Ljt",
        "outputId": "82a25d9c-062b-4718-d282-cb16d839aa3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([16, 768]), torch.Size([16, 16]))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context.shape , attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGySpDFl3Lju",
        "outputId": "9a72f319-e934-4272-962c-6b5defd4c85b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "with torch.no_grad():\n",
        "    plt.imshow(attention, interpolation='nearest')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0RaKNZY3Ljv",
        "outputId": "40a57ea1-25e6-4c09-933f-03aff38a8942"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4.1652e-05, 1.8743e-08, 4.3531e-15, 1.3554e-17, 2.0464e-16, 1.3552e-08,\n",
              "         9.9992e-01, 1.8685e-20, 2.9881e-23, 3.2375e-22, 4.0948e-22, 5.8909e-23,\n",
              "         5.1022e-18, 4.1652e-05, 1.0425e-15, 3.1295e-16],\n",
              "        [6.9659e-16, 5.0016e-07, 1.9475e-12, 6.0816e-15, 4.3889e-14, 1.0717e-07,\n",
              "         1.0000e+00, 1.2967e-17, 1.6856e-19, 1.3311e-18, 2.6679e-18, 2.0186e-19,\n",
              "         6.4972e-16, 6.9659e-16, 3.3120e-14, 1.4395e-14],\n",
              "        [2.8160e-17, 4.6376e-12, 3.3828e-09, 3.1156e-10, 7.6270e-09, 3.4791e-07,\n",
              "         1.0000e+00, 1.6709e-13, 7.2155e-15, 7.7347e-15, 2.2479e-14, 1.4072e-14,\n",
              "         3.9355e-12, 2.8160e-17, 7.8200e-14, 8.5553e-14],\n",
              "        [8.9966e-22, 3.2495e-17, 1.8656e-13, 6.4120e-14, 1.5420e-11, 2.0936e-09,\n",
              "         1.0000e+00, 3.8246e-18, 1.3870e-19, 3.8302e-19, 3.0957e-18, 2.5225e-19,\n",
              "         1.7452e-17, 8.9966e-22, 3.4182e-18, 3.9053e-18],\n",
              "        [1.0318e-33, 1.2762e-28, 2.2427e-30, 7.8236e-32, 1.9107e-27, 3.6965e-22,\n",
              "         1.0000e+00, 3.1281e-36, 1.3927e-38, 2.2895e-37, 1.5183e-36, 5.2110e-38,\n",
              "         4.2408e-35, 1.0318e-33, 5.6916e-34, 2.6557e-34],\n",
              "        [2.9877e-19, 1.8721e-15, 7.1188e-10, 7.5957e-10, 9.5682e-08, 1.9970e-06,\n",
              "         1.0000e+00, 8.9723e-14, 7.0886e-15, 7.2686e-15, 2.1089e-14, 9.9914e-15,\n",
              "         2.6498e-13, 2.9877e-19, 2.0165e-15, 2.6876e-15],\n",
              "        [7.4719e-22, 5.1469e-17, 6.6067e-16, 1.9406e-16, 2.8237e-13, 1.2238e-05,\n",
              "         9.9999e-01, 2.6902e-20, 6.5572e-22, 3.2489e-21, 8.4031e-21, 5.6765e-22,\n",
              "         3.0449e-20, 7.4719e-22, 4.7050e-20, 4.0429e-20],\n",
              "        [4.1652e-05, 1.8743e-08, 4.3531e-15, 1.3554e-17, 2.0464e-16, 1.3552e-08,\n",
              "         9.9992e-01, 1.8685e-20, 2.9880e-23, 3.2375e-22, 4.0949e-22, 5.8910e-23,\n",
              "         5.1022e-18, 4.1652e-05, 1.0425e-15, 3.1296e-16],\n",
              "        [5.9133e-15, 9.5168e-11, 2.9626e-09, 5.4552e-11, 8.0166e-10, 4.4084e-07,\n",
              "         1.0000e+00, 1.6482e-12, 5.3448e-14, 5.9957e-14, 2.1881e-13, 1.3077e-13,\n",
              "         1.2206e-11, 5.9133e-15, 5.7231e-07, 3.3679e-07],\n",
              "        [9.3829e-15, 1.1562e-10, 2.0312e-09, 3.8886e-11, 5.3428e-10, 3.4737e-07,\n",
              "         1.0000e+00, 1.2102e-12, 3.6069e-14, 4.4100e-14, 1.6976e-13, 9.6020e-14,\n",
              "         9.4887e-12, 9.3829e-15, 8.3791e-07, 5.1095e-07],\n",
              "        [5.2414e-15, 8.6201e-11, 1.4455e-09, 2.4929e-11, 3.8093e-10, 3.0065e-07,\n",
              "         1.0000e+00, 8.3030e-13, 2.2826e-14, 2.7789e-14, 1.2366e-13, 6.0846e-14,\n",
              "         5.6131e-12, 5.2414e-15, 1.0698e-06, 5.7978e-07],\n",
              "        [3.0958e-15, 4.6744e-11, 1.6457e-09, 2.8861e-11, 4.2559e-10, 2.4767e-07,\n",
              "         1.0000e+00, 8.3148e-13, 2.3863e-14, 2.9668e-14, 1.1722e-13, 6.3246e-14,\n",
              "         5.7814e-12, 3.0958e-15, 9.5961e-07, 4.9884e-07],\n",
              "        [1.8006e-14, 1.9729e-10, 1.4437e-09, 2.0318e-11, 2.4315e-10, 2.3768e-07,\n",
              "         1.0000e+00, 7.8998e-13, 1.9282e-14, 2.3731e-14, 1.0645e-13, 5.7221e-14,\n",
              "         6.2588e-12, 1.8006e-14, 2.6341e-06, 1.4341e-06],\n",
              "        [8.4964e-15, 1.3390e-10, 1.1086e-09, 1.4253e-11, 1.9078e-10, 2.0294e-07,\n",
              "         1.0000e+00, 6.0238e-13, 1.3977e-14, 1.6624e-14, 7.8173e-14, 3.9471e-14,\n",
              "         4.1144e-12, 8.4965e-15, 2.1830e-06, 1.1469e-06],\n",
              "        [2.4448e-14, 2.9755e-10, 1.9748e-09, 2.3722e-11, 2.6664e-10, 2.9164e-07,\n",
              "         9.9999e-01, 1.1013e-12, 2.8862e-14, 3.5822e-14, 1.4386e-13, 8.3851e-14,\n",
              "         9.6447e-12, 2.4448e-14, 4.6905e-06, 2.2014e-06],\n",
              "        [1.8272e-14, 2.1756e-10, 1.5898e-09, 2.3002e-11, 2.6720e-10, 2.2422e-07,\n",
              "         1.0000e+00, 9.2031e-13, 2.4502e-14, 3.0381e-14, 1.2275e-13, 7.0096e-14,\n",
              "         7.7049e-12, 1.8272e-14, 2.8986e-06, 1.6224e-06]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9upZg7Ge3Ljw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9 (default, Jun 29 2022, 11:45:57) \n[GCC 8.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09335cf1773b4a85a0473975a37d0bce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14562548596c44e599ca871259b5ea6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d1460f426b8489fb479f16ab0e12866",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_24d413de422a43228d22fe615c7666d6",
            "value": "100%"
          }
        },
        "24d413de422a43228d22fe615c7666d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b74ed454e0745ed89ad9371a5a65336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3923065cc1534667bd96088e69709aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3cdc1c3e278c4c37990a9c7cfce1421e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d1460f426b8489fb479f16ab0e12866": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "443a4cdd89194c3696ac5cb18b6a9328": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d65d5afc12c49ceb046ea73687388cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14562548596c44e599ca871259b5ea6e",
              "IPY_MODEL_cff01ce8b1f94d8a8d152d91d4ab7010",
              "IPY_MODEL_add5004177c1484eb8e07a85ddf05b08"
            ],
            "layout": "IPY_MODEL_c2578573080b45ef81cd825729746cda"
          }
        },
        "5a529bbb865f4cc6835de492b0465b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6787942a897f4169a067a72fd3974541": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "722a7de74690427e9a2538a6965a6e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7991b551ea8d4e2a947f9f0f750e96d2",
              "IPY_MODEL_fc536b7ff46e4081b1762d33a1637532",
              "IPY_MODEL_b953a54e64a74078bedd3aae52f0e258"
            ],
            "layout": "IPY_MODEL_c22ca34e00364120a7b2180b9518e6ed"
          }
        },
        "7991b551ea8d4e2a947f9f0f750e96d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6f7cb84494d4491b28ed179a2e62711",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6787942a897f4169a067a72fd3974541",
            "value": "100%"
          }
        },
        "a263e7081ca44bb1bb48273a940f386e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add5004177c1484eb8e07a85ddf05b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09335cf1773b4a85a0473975a37d0bce",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5a529bbb865f4cc6835de492b0465b6e",
            "value": " 125/125 [00:29&lt;00:00,  4.55ba/s]"
          }
        },
        "b953a54e64a74078bedd3aae52f0e258": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cdc1c3e278c4c37990a9c7cfce1421e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a263e7081ca44bb1bb48273a940f386e",
            "value": " 633/633 [02:36&lt;00:00,  4.24ba/s]"
          }
        },
        "c22ca34e00364120a7b2180b9518e6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2578573080b45ef81cd825729746cda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff01ce8b1f94d8a8d152d91d4ab7010": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea9fcf6a5a044df69c4c28afa41dc75a",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b74ed454e0745ed89ad9371a5a65336",
            "value": 125
          }
        },
        "e6f7cb84494d4491b28ed179a2e62711": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea9fcf6a5a044df69c4c28afa41dc75a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc536b7ff46e4081b1762d33a1637532": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_443a4cdd89194c3696ac5cb18b6a9328",
            "max": 633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3923065cc1534667bd96088e69709aca",
            "value": 633
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
