{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"akBumB45ABYa","outputId":"bb105322-fa68-4885-ef0f-bf8e3478e02d","executionInfo":{"status":"ok","timestamp":1665676620189,"user_tz":-210,"elapsed":4435,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!git config --global user.name \"alistvt\"\n","!git config --global user.email \"alistvt@gmail.com\"\n","\n","%cd /content/drive/MyDrive/multidoc-conv-qa/src/retriever"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dvS3v9W3m7t","outputId":"e1e89057-fb38-43c8-b6e0-bd3334dfc5bd","executionInfo":{"status":"ok","timestamp":1665676620190,"user_tz":-210,"elapsed":14,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/multidoc-conv-qa/src/retriever\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCHw5gxO0Jx-"},"outputs":[],"source":["!pip install -r ../../requirements.txt --quiet\n","!pip install --quiet transformers datasets accelerate"]},{"cell_type":"markdown","metadata":{"id":"5akM0VXmzYku"},"source":["# Document Retrival with Follow up Detector (DR.FUD) + DR. TEIT\n","\n","In this method we use a FCN to detect wheter a question is a follow up of the previous question, meaning that the document is the same of not. If the document is the same, we use the previous answer's document for this question also.\n","\n","We used LaBSE model for out embeddings. For computing title embedding similarities we used cosine similarity between query embeddings and each document's title embedding."]},{"cell_type":"markdown","metadata":{"id":"i9vpLyUkzWzO"},"source":["# Dataset\n","### Dataset Description\n","\n","- **mutldoc2dial_doc.json** contains the documents that are indexed by key `domain` and `doc_id` . Each document instance includes the following,\n","\n","  - `doc_id`: the ID of a document;\n","  - `title`: the title of the document;\n","  - `domain`: the domain of the document;\n","  - `doc_text`: the text content of the document (without HTML markups);\n","  - `doc_html_ts`: the document content with HTML markups and the annotated spans that are indicated by `text_id` attribute, which corresponds to `id_sp`.\n","  - `doc_html_raw`: the document content with HTML markups and without span annotations.\n","  - `spans`: key-value pairs of all spans in the document, with `id_sp` as key. Each span includes the following,\n","    - `id_sp`: the id of a  span as noted by `text_id` in  `doc_html_ts`;\n","    - `start_sp`/  `end_sp`: the start/end position of the text span in `doc_text`;\n","    - `text_sp`: the text content of the span.\n","    - `id_sec`: the id of the (sub)section (e.g. `<p>`) or title (`<h2>`) that contains the span.\n","    - `start_sec` / `end_sec`: the start/end position of the (sub)section in `doc_text`.\n","    - `text_sec`: the text of the (sub)section.\n","    - `title`: the title of the (sub)section.\n","    - `parent_titles`: the parent titles of the `title`.\n","\n","- **multidoc2dial_dial_train.json** and **multidoc2dial_dial_validation.json**  contain the training and dev split of dialogue data that are indexed by key `domain` . Please note: **For test split, we only include a dummy file in this version.**\n","\n","  Each dialogue instance includes the following,\n","\n","  - `dial_id`: the ID of a dialogue;\n","  - `turns`: a list of dialogue turns. Each turn includes,\n","    - `turn_id`: the time order of the turn;\n","    - `role`: either \"agent\" or \"user\";READ\n","    - `da`: dialogue act;\n","    - `references`: a list of spans with `id_sp` ,  `label` and `doc_id`. `references` is empty if a turn is for indicating previous user query not answerable or irrelevant to the document. **Note** that labels \"*precondition*\"/\"*solution*\" are fuzzy annotations that indicate whether a span is for describing a conditional context or a solution.\n","    - `utterance`: the human-generated utterance based on the dialogue scene.\n","Downloading the training dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEuwmS7O03CH"},"outputs":[],"source":["import json\n","with open('../../dataset/multidoc2dial/v1.0/multidoc2dial_doc.json', 'r') as f:\n","    multidoc2dial_doc = json.load(f)"]},{"cell_type":"markdown","metadata":{"id":"i2Wa65Cl1xUF"},"source":["#### Extracting titles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0D-IV5H0vE4"},"outputs":[],"source":["titles = []\n","for doc_idx1 in multidoc2dial_doc['doc_data']:\n","    for doc_idx2 in multidoc2dial_doc['doc_data'][doc_idx1]:\n","        titles.append(doc_idx2)"]},{"cell_type":"markdown","metadata":{"id":"ng4nYreI15Vn"},"source":["#### Extracting document texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96D34GnE1wQL"},"outputs":[],"source":["doc_texts_train = []\n","title_to_domain = {}\n","title_to_text = {}\n","for doc_idx1 in multidoc2dial_doc['doc_data']:\n","    for doc_idx2 in multidoc2dial_doc['doc_data'][doc_idx1]:\n","        doc_texts_train.append(multidoc2dial_doc['doc_data'][doc_idx1]\\\n","                                          [doc_idx2]['doc_text'].strip())\n","        title_to_domain[doc_idx2] = doc_idx1\n","        title_to_text[doc_idx2] = multidoc2dial_doc['doc_data'][doc_idx1][doc_idx2]['doc_text'].strip()"]},{"cell_type":"markdown","metadata":{"id":"n4UoewSm0ICB"},"source":["## Encoding the sentences\n","We use the LaBSE which is a Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1HTJrcK0RpF"},"outputs":[],"source":["from transformers import (AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification,\n","                          LongformerForSequenceClassification, LongformerTokenizerFast)\n","import numpy as np\n","import torch\n","from torch.nn.functional import normalize\n","\n","from tqdm import tqdm\n"]},{"cell_type":"code","source":["model_name = \"setu4993/LaBSE\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)"],"metadata":{"id":"okfHuMY4987O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ngG9eTIj0c01"},"source":["### `get_embeddings`\n","In this method we extract the **pooler output** (Last layer hidden-state of the first token of the sequence (classification token) after further processing through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns the classification token after processing through a linear layer and a tanh activation function. The linear layer weights are trained from the next sentence prediction (classification) objective during pretraining)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCTH88_t0ZD0"},"outputs":[],"source":["def get_embeddings(sentece):\n","    \"\"\"\n","    Return embeddings based on encoder model\n","\n","    :param sentence: input sentence(s)\n","    :type sentence: str or list of strs\n","    :return: embeddings\n","    \"\"\"\n","    tokenized = tokenizer(sentece,\n","                                return_tensors=\"pt\",\n","                                padding=True)\n","    with torch.no_grad():\n","        embeddings = model(**tokenized)\n","    \n","    return np.squeeze(np.array(embeddings.pooler_output))"]},{"cell_type":"markdown","metadata":{"id":"q_VqBSNi4AaD"},"source":["### Title embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hunKGBFq4C76"},"outputs":[],"source":["import os\n","title_embeddings_file = 'doc_title_LaBSE_Embedding.npy'\n","\n","if not os.path.exists(title_embeddings_file):\n","    title_embeddings = []\n","    for title in tqdm(titles):\n","        title_embeddings.append(get_embeddings(title))\n","\n","    with open(title_embeddings_file, 'wb') as f:\n","        np.save(f, np.array(title_embeddings))\n","else:\n","    title_embeddings = np.load(title_embeddings_file)\n","    title_embeddings = list(title_embeddings)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQmy3UiYiGG8"},"outputs":[],"source":["import pickle\n","title_to_embeddings_file = 'title_to_embeddings.pkl'\n","\n","if not os.path.exists(title_to_embeddings_file):\n","    title_to_embeddings = {}\n","    for title in tqdm(titles):\n","        title_to_embeddings[title] = get_embeddings(title)\n","    with open(title_to_embeddings_file, 'wb') as f:\n","        pickle.dump(title_to_embeddings, f)\n","else:\n","    with open(title_to_embeddings_file, 'rb') as f:\n","        title_to_embeddings = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"d42w3h1C2V1Q"},"source":["## Calculating the IDF for each token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjH-jRgs0fyv"},"outputs":[],"source":["words_idf_file = 'IDFs.pkl'\n","N_doc = len(doc_texts_train)\n","\n","if not os.path.exists(words_idf_file):\n","    # First getting all distinct words in all documents\n","    words = set()\n","    doc_texts_train_tokenized = []\n","    for doc in tqdm(doc_texts_train, desc=\"getting all words from documents\"):\n","        tokenized_doc = [s.lower() for s in tokenizer.tokenize(doc)]\n","        doc_texts_train_tokenized.append(tokenized_doc) \n","        words = set(tokenized_doc).union(words)\n","\n","    # calculating each word IDF\n","    words2IDF = {}\n","    for word in tqdm(words, desc=\"calculating words IDF scores\"):\n","        n_word = 0\n","        for doc in doc_texts_train_tokenized:\n","            if word in doc:\n","                n_word += 1\n","        words2IDF[word] = np.log(N_doc / (n_word + 1))\n","\n","    with open(words_idf_file, 'wb') as f:\n","        pickle.dump(words2IDF, f)\n","\n","else:\n","    with open(words_idf_file, 'rb') as f:\n","        words2IDF = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6blMmmk2ugT","outputId":"9b1f7f15-f7f7-4438-fe3e-e362541c5542","executionInfo":{"status":"ok","timestamp":1665676684761,"user_tz":-210,"elapsed":5,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["8446"]},"metadata":{},"execution_count":12}],"source":["len(words2IDF)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfcYwbdJ2zSN"},"outputs":[],"source":["def calc_idf_score(sentence):\n","    \"\"\"\n","    Calculate the mean idf score for given sentence.\n","    (used to understand the contribution of the knowledge of each question\n","    questions with high frequent words are meaningless and we can ignore them\n","    roughly, which is done by this score.)\n","\n","    :param sentence: input sentence\n","    :type sentence: str\n","    :return: mean idf score of sentence token\n","    \"\"\"\n","    tokenzied_sentence = [s.lower() for s in tokenizer.tokenize(sentence)]\n","    score = 0\n","    for token in tokenzied_sentence:\n","        if token in words2IDF:\n","            score += words2IDF[token]\n","        else:\n","            score += np.log(N_doc)\n","    return score / len(tokenzied_sentence) if len(tokenzied_sentence) else 0"]},{"cell_type":"markdown","metadata":{"id":"KtOJcbeo5rBQ"},"source":["# Methods"]},{"cell_type":"markdown","metadata":{"id":"Wr3Qbox83Ljc"},"source":["## FCN based on [cls]"]},{"cell_type":"markdown","metadata":{"id":"_zRT_9kl3Ljd"},"source":["### AutoModelForSequenceClassification"]},{"cell_type":"markdown","source":["We use a classification method on the questions to decide the relationship between the previous question and current question.\n","In the dataset provided to us, previous turn documents are predefined, meaning that we are aware of the previous documents, therefore, if the prediction predicts that current question's document is the same as the previous, we don't need to retrieve a document and we give the previous doc_id."],"metadata":{"id":"0oFuQ6Tq8eoC"}},{"cell_type":"markdown","metadata":{"id":"hsHE7D873Ljd"},"source":["#### model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRkLfGS43Lje"},"outputs":[],"source":["tokenizer_name = \"setu4993/LaBSE\"\n","model_name = \"alistvt/fudnet\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n","fudnet_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","device = torch.device(\"cuda:0\")\n","fudnet_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"pcI4P0Lt3Lje"},"source":["#### metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgLwYYuQ3Ljf"},"outputs":[],"source":["import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"markdown","metadata":{"id":"kuhM4K4J1E3Y"},"source":["### DR. TEIT*\n","\n","In this method we used two scoring measure and aggregate them by a convex combination as below:\n","$$\n","λ*Similiarity_{Title Embedding} + (1-λ)*Similiarity_{TextIDF}\n","$$\n","\n","We used LaBSE model for our embeddings. For computing title embedding similarities we used cosine similarity between query embeddings and each document's title embedding.\n","\n","For the second part we used character-level (2gram to 8gram). We also trained our TF-IDF transformation matrix on the Multidoc2dial2022 documnets.\n","\n","**NOTE: In `predict_DR_TEIT` you may see a diffrent notation (`alpha`) but they are the same.**"]},{"cell_type":"markdown","metadata":{"id":"q251xG1B1f2I"},"source":["#### TF-IDF Transformation Matrix Fitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoNS99AY1LZl"},"outputs":[],"source":["doc_texts_train = []\n","for doc_idx1 in multidoc2dial_doc['doc_data']:\n","    for doc_idx2 in multidoc2dial_doc['doc_data'][doc_idx1]:\n","        doc_texts_train.append(multidoc2dial_doc['doc_data'][doc_idx1]\\\n","                                          [doc_idx2]['doc_text'].strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZs4IB7x1S68"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidfVectorizer = TfidfVectorizer(strip_accents=None,\n","                                 analyzer='char',\n","                                 ngram_range=(2, 8),\n","                                 norm='l2',\n","                                 use_idf=True,\n","                                 smooth_idf=True)\n","tfidf_wm = tfidfVectorizer.fit_transform(doc_texts_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3p34wgW1emS"},"outputs":[],"source":["import pickle\n","with open('tfidfVectorizer.pkl', 'wb') as f:\n","    pickle.dump(tfidfVectorizer, f)\n","\n","with open('tfidf_wm.pkl', 'wb') as f:\n","    pickle.dump(tfidf_wm, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VU-xRf0V3Ljl","outputId":"2502e0fe-d2de-4461-9efe-e5bd2a84583e","executionInfo":{"status":"ok","timestamp":1665598720478,"user_tz":-210,"elapsed":723,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1047632"]},"metadata":{},"execution_count":20}],"source":["len(tfidfVectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"30VbDrMH15l6"},"source":["#### DR. TEIT\n","\n","the input is consisted of a list of queries, which is the current question and its history turns.\n","for each of the questions, we compute two similarity score for each of our documents, one of them is based on the pretrained LM and the other on is based on character level matching. Both of these scores will be weighted by a coefficient which is the `idf_score` of the query, defining how much meaning does the query contain. Then these scores will be summed up in a convex manner and the final matching score with all documents is computed. We return the result by sorting these scores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ELrMT4g17vc"},"outputs":[],"source":["def predict_DR_TEIT(queries, k=1, alpha=10):\n","    \"\"\"\n","    Predict which document is matched to the given query.\n","\n","    :param queries: input queries in time reversed order (latest first)\n","    :type queries: str (or list of strs)\n","    :param k: number of returning docs\n","    :type k: int \n","    :return: return the document names and accuracies\n","    \"\"\"\n","\n","    idf_score = np.array(list(map(lambda x: 0.0, title_embeddings)))\n","    tfidf_score = np.array(list(map(lambda x: 0.0, title_embeddings)))\n","    coef_sum = 0\n","    for i, query in enumerate(queries):\n","        query_embd = get_embeddings(query)\n","        query_sim = list(map(lambda x: np.dot(x, query_embd) /\n","                            (np.linalg.norm(query_embd) * np.linalg.norm(x)),\n","                            title_embeddings))\n","        query_sim = np.array(query_sim)\n","        # coef = 2**(-i) * calc_idf_score(query)\n","        coef = calc_idf_score(query)\n","        coef_sum += coef\n","\n","        idf_score += coef * query_sim\n","        tfidf_score += coef * np.squeeze(np.asarray(tfidf_wm @ tfidfVectorizer.transform([query]).todense().T))\n","\n","    scores = (idf_score + alpha * tfidf_score) / coef_sum\n","    best_k_idx = scores.argsort()[::-1][:k]\n","    scores = scores[best_k_idx]\n","    predictions = list(map(lambda x: titles[x], best_k_idx))\n","    return (scores, predictions)"]},{"cell_type":"markdown","source":["### FUDNet + DR. TEIT"],"metadata":{"id":"oPC_VWvd407M"}},{"cell_type":"code","source":["def predict_FUDNet_DR_TEIT(data, k=1):\n","    inputs = tokenize_function(data, prediction=True, cuda=True)\n","    outputs = fudnet_model(**inputs)\n","    is_followup = bool(torch.argmax(outputs.logits))\n","    \n","    if is_followup:\n","        dr_scores, dr_predictions = predict_DR_TEIT([data['prev_answer'], data['question'], data['history']], k=k)\n","        return dr_predictions\n","    else:\n","        dr_scores, dr_predictions = predict_DR_TEIT([data['question']], k=k)\n","        return dr_predictions"],"metadata":{"id":"FOIe5Tn643ZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.loc[2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YCZ4JOhRoyQ","outputId":"3d50638f-8a86-4f58-a4ab-03e2ae65de14","executionInfo":{"status":"ok","timestamp":1665582582520,"user_tz":-210,"elapsed":5,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["history                         Don't do that I'll get insurance\n","question       I have, that is why I am here to clear that up...\n","combined       Don't do that I'll get insurance <SEP> I have,...\n","followup                                                       1\n","prev_doc            Top 5 DMV Mistakes and How to Avoid Them#3_0\n","current_doc         Top 5 DMV Mistakes and How to Avoid Them#3_0\n","prev_answer    Okay, have you received a letter from the DMV ...\n","Name: 2, dtype: object"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["predict_FUDNet_DR_TEIT(test_df.loc[2], k=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlqEPT-y_Jtb","outputId":"e54d3192-67a9-435e-a5a1-9d133df12bc1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Top 5 DMV Mistakes and How to Avoid Them#3_0',\n"," 'Respond to DMV insurance letters and orders#3_0',\n"," 'How to change your address#1_0',\n"," 'Insurance lapses#3_0',\n"," 'Information about transaction entries#3_0']"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["## Unknown Prediction\n","here we train a Classifier to get a question and a document and predict wether the question can be answered with respect to the document."],"metadata":{"id":"IXkL2FPDd7fl"}},{"cell_type":"markdown","source":["### Constructing Unknown detection Dataset\n","we construct a dataset that gets the history and selected documents. The documents are retrieved from the `Dr_Fud_Dr_Teit` Predictor and we choose the first 3 of them and for each of them we give a label: 0 means unanswerable and 1 means answerable."],"metadata":{"id":"W3ZyvN6AeWpj"}},{"cell_type":"code","source":["def combine_history_and_doc(history, doc):\n","    separation_token = \" <SEP> \"\n","    return history + separation_token + doc\n","\n","\n","def construct_unknown_dataset(filepath):\n","    import json\n","    with open(filepath, 'r') as f:\n","        multidoc2dial_dial_train = json.load(f)\n","    \n","    historys = []\n","    doc_ids = []\n","    labels = []\n","\n","    for domain in multidoc2dial_dial_train['dial_data']:\n","        for dial in tqdm(multidoc2dial_dial_train['dial_data'][domain], desc=f\"processing domain {domain}\"):\n","            prev_question = ''\n","            prev_answer = ''\n","            for turn in dial['turns']:\n","                if turn['role'] == \"user\":\n","                    current_question = turn['utterance']\n","                    combined = combine_sentences(prev_question, current_question)\n","\n","                    inputs = tokenize_function({'combined': combined}, prediction=True, cuda=True)\n","                    outputs = fudnet_model(**inputs)\n","                    is_followup = bool(torch.argmax(outputs.logits))\n","                    \n","                    if is_followup:\n","                        history = [prev_answer, current_question, prev_question]\n","                        history_str = \" \".join(history).strip()\n","                        dr_scores, dr_predictions = predict_DR_TEIT(history, k=3)\n","                    else:\n","                        history = [current_question]\n","                        history_str = history[0].strip()\n","                        dr_scores, dr_predictions = predict_DR_TEIT(history, k=3)\n","\n","                    current_doc_id = turn['references'][0]['doc_id']\n","                    dr_predictions.append(current_doc_id)\n","                    doc_id_predictions = list(set(dr_predictions))\n","\n","                    for doc_id in doc_id_predictions:\n","                        historys.append(history_str)\n","                        doc_ids.append(doc_id)\n","                        labels.append(doc_id == current_doc_id)\n","\n","                    prev_question = current_question\n","                else:\n","                    prev_answer = turn['utterance']\n","                    \n","    return historys, doc_ids, labels"],"metadata":{"id":"yT0OO9SZe04a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","if not os.path.exists(\"notebooks/train_unknown_df.json\"):\n","    train_history, train_doc_ids, train_labels = construct_unknown_dataset('../../dataset/multidoc2dial/v1.0/multidoc2dial_dial_train.json')\n","    test_history, test_doc_ids, test_labels = construct_unknown_dataset('../../dataset/multidoc2dial/v1.0/multidoc2dial_dial_validation.json')\n","\n","    train_doc_texts = [title_to_text[title] for title in train_doc_ids]\n","    test_doc_texts = [title_to_text[title] for title in test_doc_ids]\n","\n","    train_combined = [combine_history_and_doc(_history, _doc_text) for _history, _doc_text in zip(train_history, train_doc_texts)]\n","    test_combined = [combine_history_and_doc(_history, _doc_text) for _history, _doc_text in zip(test_history, test_doc_texts)]\n","\n","    train_unknown_df = pd.DataFrame({\n","        'history': train_history,\n","        'doc_text': train_doc_texts,\n","        'combined': train_combined,\n","        'label': train_labels\n","    })\n","    test_unknown_df = pd.DataFrame({\n","        'history': test_history,\n","        'doc_text': test_doc_texts,\n","        'combined': test_combined,\n","        'label': test_labels\n","    })\n","    \n","    with open(\"notebooks/train_unknown_df.json\", 'w', encoding=\"utf-8\") as f:\n","        train_unknown_df.to_json(f)\n","    with open(\"notebooks/test_unknown_df.json\", 'w', encoding=\"utf-8\") as f:\n","        test_unknown_df.to_json(f)\n","else:\n","    train_unknown_df = pd.read_json(\"notebooks/train_unknown_df.json\")\n","    test_unknown_df = pd.read_json(\"notebooks/test_unknown_df.json\")\n","    \n"],"metadata":{"id":"4iGLfNzflrjQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = []\n","total = 0\n","for doc in title_to_text.values():\n","    s.append(len(doc.split()))\n","\n","import numpy as np\n","\n","#calculate standard deviation of list\n","print(np.mean(s))\n","print(np.std(s))\n","\n","len(train_unknown_df.loc[train_unknown_df['label'] == True])/len(train_unknown_df.loc[train_unknown_df['label'] == False])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XN15KKzaN80G","executionInfo":{"status":"ok","timestamp":1665676701722,"user_tz":-210,"elapsed":4,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}},"outputId":"92e2fea6-b937-4fea-d801-58d296f250de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["828.3852459016393\n","624.3178434246311\n"]},{"output_type":"execute_result","data":{"text/plain":["0.4357923751699477"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["\n","s = []\n","total = 0\n","for doc in train_unknown_df['history']:\n","    s.append(len(doc.split()))\n","\n","import numpy as np\n","\n","#calculate standard deviation of list\n","print(np.mean(s))\n","print(np.std(s))\n","\n","len(train_unknown_df.loc[train_unknown_df['label'] == True])/len(train_unknown_df.loc[train_unknown_df['label'] == False])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovU38hwuBngX","executionInfo":{"status":"ok","timestamp":1665673749661,"user_tz":-210,"elapsed":20,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}},"outputId":"1ea63ae5-f95c-41ef-a031-2c1ee0373dd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["30.910898666528304\n","17.90193085542348\n"]},{"output_type":"execute_result","data":{"text/plain":["0.4357923751699477"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["!git status"],"metadata":{"id":"eUaOZY7ZCmqw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665673876670,"user_tz":-210,"elapsed":442,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}},"outputId":"122676b6-ffb7-41b5-c51b-1b335bc20d66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch concat-documents\n","Your branch is up to date with 'origin/concat-documents'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mIDFs.pkl\u001b[m\n","\t\u001b[31mdoc_title_LaBSE_Embedding.npy\u001b[m\n","\t\u001b[31mnotebooks/DR_FUD_Unknown.ipynb\u001b[m\n","\t\u001b[31mnotebooks/test_unknown_df.json\u001b[m\n","\t\u001b[31mnotebooks/train_unknown_df.json\u001b[m\n","\t\u001b[31mtfidfVectorizer.pkl\u001b[m\n","\t\u001b[31mtfidf_wm.pkl\u001b[m\n","\t\u001b[31mtitle_to_embeddings.pkl\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]},{"cell_type":"code","source":["tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)\n","\n","def tokenize_function(examples, prediction=False, cuda=False, max_length=1024):\n","    if prediction:\n","        tokenized = tokenizer(examples['combined'], max_length=max_length, padding=\"max_length\", truncation=True, return_tensors='pt')\n","    else:\n","        tokenized = tokenizer(examples['combined'], max_length=max_length, padding=\"max_length\", truncation=True)\n","    if cuda:\n","        tokenized_cuda = {}\n","        for key, value in tokenized.items():\n","            tokenized_cuda[key] = value.cuda()\n","        return tokenized_cuda\n","    else:\n","        return tokenized"],"metadata":{"id":"QKa6z_m-wLNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset, DatasetDict\n","\n","train_unknown_dataset = Dataset.from_pandas(train_unknown_df)\n","test_unknown_dataset = Dataset.from_pandas(test_unknown_df)\n","\n","tokenized_unknown_trainset = train_unknown_dataset.map(tokenize_function, batched=True)\n","tokenized_unknown_testset = test_unknown_dataset.map(tokenize_function, batched=True)\n","\n","unknown_dataset = DatasetDict()\n","\n","unknown_dataset['train'] = tokenized_unknown_trainset\n","unknown_dataset['validation'] = tokenized_unknown_testset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["84dc93c342a84942b82ebf6075aeda07","6965c6e725b34188ae3137fa7eac33a3","0372fe2e9a22479d94d7548fb2d5501d","a1441a04ba044667b3ac6d43d31f37c2","9ab3c293d7eb4ee0abafe4aeb4effb74","e4dbfc5d33af424babbbf2e75cb7f78d","99ddc0083bc4487b90f6bea792d75c07","23c843c0afa544f48e802b858ad9eaad","a28213d2b6f544ef95b008da78e383f8","4c44e38f6a1a490eb8a2446a2e52ba69","a842013d7ebb4c7187832e548ff75086","9d599381394c44f7bd447013033c30a2","506574c2dc5a4c21b4a982a09f7e1cfc","6c8c301534e24e5d957aec3b83636204","6700930c83b34ffb8e0b9aa13ac70013","975f64047561442a9b63f61ee47507a6","c86a16d488ec407dacca60690e9eaf06","0f100ba0114b4366bff0b28e91f57b76","7dfa8b12b1174f518414c84ddc3e842c","afde3cf3365c411c8fb2152794db2b6c","edcf7d01f07a43b4801894af5227f951","547b4391c4c14231964f32fbfb4efd79"]},"id":"_oA2HiJ-rfM3","executionInfo":{"status":"ok","timestamp":1665676793697,"user_tz":-210,"elapsed":84588,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}},"outputId":"5c273b2b-0237-4d2a-fbb7-77745352f625"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f7397d73c20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/78 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84dc93c342a84942b82ebf6075aeda07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/15 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d599381394c44f7bd447013033c30a2"}},"metadata":{}}]},{"cell_type":"markdown","source":["### Model and Training"],"metadata":{"id":"b2WAMtkPwhLF"}},{"cell_type":"code","source":["unknown_predictor_model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n","                                                           gradient_checkpointing=False,\n","                                                           attention_window = 512,\n","                                                           num_labels=2)\n","\n","device = torch.device(\"cuda:0\")\n","unknown_predictor_model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7GtpLNl6ilP","executionInfo":{"status":"ok","timestamp":1665676797063,"user_tz":-210,"elapsed":3392,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}},"outputId":"6ee4395c-d973-4d75-b6f3-991e3eb1dfe1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["LongformerForSequenceClassification(\n","  (longformer): LongformerModel(\n","    (embeddings): LongformerEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): LongformerEncoder(\n","      (layer): ModuleList(\n","        (0): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): LongformerLayer(\n","          (attention): LongformerAttention(\n","            (self): LongformerSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): LongformerSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): LongformerIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): LongformerOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): LongformerClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='/home/',\n","    num_train_epochs=2,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    warmup_steps=50,\n","    weight_decay=0.01,\n","    logging_steps=0.01,\n","    fp16 = True,\n","    evaluation_strategy='epoch',\n","    save_strategy ='epoch',\n","    load_best_model_at_end=True,\n","    dataloader_num_workers = 0\n",")\n","\n","trainer = Trainer(\n","    model=unknown_predictor_model,\n","    args=training_args,\n","    train_dataset=unknown_dataset['train'],\n","    eval_dataset=unknown_dataset['validation'],\n","    compute_metrics=compute_metrics\n",")"],"metadata":{"id":"aYnYcGpWwlDA","executionInfo":{"status":"ok","timestamp":1665677027100,"user_tz":-210,"elapsed":620,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a8065cba-22b0-4315-c895-b327804ac78a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using amp half precision backend\n"]}]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hny9ekqfWsi9","outputId":"57a15075-33e8-4153-effe-7a063605d0a0","executionInfo":{"status":"error","timestamp":1665683046769,"user_tz":-210,"elapsed":226517,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='19355' max='38546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [19355/38546 1:40:14 < 1:39:23, 3.22 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.615596</td>\n","      <td>0.697992</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Saving model checkpoint to /home/checkpoint-19273\n","Configuration saved in /home/checkpoint-19273/config.json\n","Model weights saved in /home/checkpoint-19273/pytorch_model.bin\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n","Initializing global attention on CLS token...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1897\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1898\u001b[0m         )\n\u001b[1;32m   1899\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         )\n\u001b[1;32m   1716\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m                 )\n\u001b[1;32m   1298\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         )\n\u001b[1;32m   1222\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m         )\n\u001b[1;32m   1158\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0mis_index_global_attn_nonzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn_nonzero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0mis_local_index_no_global_attn_nonzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_local_index_no_global_attn_nonzero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                 \u001b[0mis_index_masked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_masked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m             )\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_compute_global_attn_output_from_hidden\u001b[0;34m(self, hidden_states, max_num_global_attn_indices, layer_head_mask, is_local_index_global_attn_nonzero, is_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero, is_index_masked)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         ], f\"global_attn_scores have the wrong size. Size should be {(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)}, but is {global_attn_scores.size()}.\"\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0mglobal_attn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_attn_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_global_attn_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         global_attn_scores[\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"et4k0Vhg9aRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H32jwXIw5KIw"},"source":["# Test\n","In the test dataset we just picked ones with **user** turn."]},{"cell_type":"code","source":["def test_loop(df, predictor):\n","    prec_at_50 = 0\n","    prec_at_10 = 0\n","    prec_at_5 = 0\n","    prec_at_3 = 0\n","    prec_at_1 = 0\n","    ranks = []\n","    for index, data in tqdm(df.iterrows()):\n","        predictions = predictor(data, k=500)\n","        actual_doc = data['current_doc']\n","        ranks.append(1 / (predictions.index(actual_doc) + 1))\n","        if actual_doc == predictions[0]:\n","            prec_at_1 += 1\n","        if actual_doc in predictions[:3]:\n","            prec_at_3 += 1\n","        if actual_doc in predictions[:5]:\n","            prec_at_5 += 1\n","        if actual_doc in predictions[:10]:\n","            prec_at_10 += 1\n","        if actual_doc in predictions[:50]:\n","            prec_at_50 += 1\n","\n","        if index % 100 == 99:\n","            print(f\"\"\"\n","                MRR: mean={np.array(ranks).mean()}, var={np.array(ranks).var()}\n","                Prec@(1) = {prec_at_1 / index}\n","                Prec@(3) = {prec_at_3 / index}\n","                Prec@(5) = {prec_at_5 / index}\n","                Prec@(10) = {prec_at_10 / index}\n","                Prec@(50) = {prec_at_50 / index}\n","                NUMBER_OF_SAMPLES = {index}\n","            \"\"\")\n","\n","    return f\"\"\"\n","        MRR: mean={np.array(ranks).mean()}, var={np.array(ranks).var()}\n","        Prec@(1) = {prec_at_1 / index}\n","        Prec@(3) = {prec_at_3 / index}\n","        Prec@(5) = {prec_at_5 / index}\n","        Prec@(10) = {prec_at_10 / index}\n","        Prec@(50) = {prec_at_50 / index}\n","        NUMBER_OF_SAMPLES = {index}\n","    \"\"\""],"metadata":{"id":"STHinF2c558c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kbiRFDJg2QP6","outputId":"af752d66-6e5b-4dd5-b502-44092dab4005","executionInfo":{"status":"error","timestamp":1665584750582,"user_tz":-210,"elapsed":2049091,"user":{"displayName":"Doctor Code","userId":"17469507786654276327"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100it [00:55,  2.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7039079215980597, var=0.14377394068348706\n","                Prec@(1) = 0.6060606060606061\n","                Prec@(3) = 0.7575757575757576\n","                Prec@(5) = 0.8181818181818182\n","                Prec@(10) = 0.9292929292929293\n","                Prec@(50) = 1.0\n","                NUMBER_OF_SAMPLES = 99\n","            \n"]},{"output_type":"stream","name":"stderr","text":["200it [01:57,  1.97it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7902209363599058, var=0.11812314929333972\n","                Prec@(1) = 0.7135678391959799\n","                Prec@(3) = 0.8492462311557789\n","                Prec@(5) = 0.8793969849246231\n","                Prec@(10) = 0.9346733668341709\n","                Prec@(50) = 0.9899497487437185\n","                NUMBER_OF_SAMPLES = 199\n","            \n"]},{"output_type":"stream","name":"stderr","text":["300it [03:00,  1.45it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7867803098563075, var=0.11421799672230108\n","                Prec@(1) = 0.6956521739130435\n","                Prec@(3) = 0.8595317725752508\n","                Prec@(5) = 0.8929765886287625\n","                Prec@(10) = 0.9331103678929766\n","                Prec@(50) = 0.9866220735785953\n","                NUMBER_OF_SAMPLES = 299\n","            \n"]},{"output_type":"stream","name":"stderr","text":["400it [03:58,  1.91it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7874319412514239, var=0.11441718060672462\n","                Prec@(1) = 0.6967418546365914\n","                Prec@(3) = 0.8571428571428571\n","                Prec@(5) = 0.899749373433584\n","                Prec@(10) = 0.9323308270676691\n","                Prec@(50) = 0.9799498746867168\n","                NUMBER_OF_SAMPLES = 399\n","            \n"]},{"output_type":"stream","name":"stderr","text":["501it [04:45,  3.00it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7901449781945575, var=0.11017359938724726\n","                Prec@(1) = 0.6933867735470942\n","                Prec@(3) = 0.8697394789579158\n","                Prec@(5) = 0.9078156312625251\n","                Prec@(10) = 0.9378757515030061\n","                Prec@(50) = 0.9819639278557114\n","                NUMBER_OF_SAMPLES = 499\n","            \n"]},{"output_type":"stream","name":"stderr","text":["600it [05:31,  2.00it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7897473916759583, var=0.10947635696873903\n","                Prec@(1) = 0.6911519198664441\n","                Prec@(3) = 0.8697829716193656\n","                Prec@(5) = 0.9081803005008348\n","                Prec@(10) = 0.9382303839732888\n","                Prec@(50) = 0.9833055091819699\n","                NUMBER_OF_SAMPLES = 599\n","            \n"]},{"output_type":"stream","name":"stderr","text":["700it [06:30,  2.25it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7726096881167444, var=0.1167908104255042\n","                Prec@(1) = 0.670958512160229\n","                Prec@(3) = 0.8555078683834049\n","                Prec@(5) = 0.8969957081545065\n","                Prec@(10) = 0.9284692417739628\n","                Prec@(50) = 0.9785407725321889\n","                NUMBER_OF_SAMPLES = 699\n","            \n"]},{"output_type":"stream","name":"stderr","text":["800it [07:14,  2.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7649304655685384, var=0.12250992626019358\n","                Prec@(1) = 0.6670838548185232\n","                Prec@(3) = 0.8410513141426783\n","                Prec@(5) = 0.8823529411764706\n","                Prec@(10) = 0.9224030037546934\n","                Prec@(50) = 0.9737171464330413\n","                NUMBER_OF_SAMPLES = 799\n","            \n"]},{"output_type":"stream","name":"stderr","text":["900it [07:58,  2.11it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7625521208615919, var=0.12496102112342923\n","                Prec@(1) = 0.6674082313681868\n","                Prec@(3) = 0.8364849833147943\n","                Prec@(5) = 0.8776418242491657\n","                Prec@(10) = 0.917686318131257\n","                Prec@(50) = 0.9699666295884316\n","                NUMBER_OF_SAMPLES = 899\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1000it [08:42,  2.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7598912916469895, var=0.12665441140332098\n","                Prec@(1) = 0.6656656656656657\n","                Prec@(3) = 0.8328328328328328\n","                Prec@(5) = 0.8738738738738738\n","                Prec@(10) = 0.914914914914915\n","                Prec@(50) = 0.96996996996997\n","                NUMBER_OF_SAMPLES = 999\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1100it [09:23,  2.48it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7614785252607713, var=0.1285055089882269\n","                Prec@(1) = 0.6724294813466788\n","                Prec@(3) = 0.8289353958143767\n","                Prec@(5) = 0.8698817106460418\n","                Prec@(10) = 0.910828025477707\n","                Prec@(50) = 0.9681528662420382\n","                NUMBER_OF_SAMPLES = 1099\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1200it [10:06,  2.36it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7538556721574035, var=0.12990768801222546\n","                Prec@(1) = 0.6605504587155964\n","                Prec@(3) = 0.8256880733944955\n","                Prec@(5) = 0.8707256046705588\n","                Prec@(10) = 0.9107589658048374\n","                Prec@(50) = 0.9658048373644704\n","                NUMBER_OF_SAMPLES = 1199\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1300it [10:49,  2.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7542757415017165, var=0.13174692449725176\n","                Prec@(1) = 0.6643571978444958\n","                Prec@(3) = 0.8229407236335643\n","                Prec@(5) = 0.8668206312548113\n","                Prec@(10) = 0.9045419553502695\n","                Prec@(50) = 0.9599692070823711\n","                NUMBER_OF_SAMPLES = 1299\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1400it [11:37,  2.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7395792539749166, var=0.1363833477441674\n","                Prec@(1) = 0.6454610436025733\n","                Prec@(3) = 0.810578984989278\n","                Prec@(5) = 0.8598999285203717\n","                Prec@(10) = 0.8970693352394568\n","                Prec@(50) = 0.9571122230164403\n","                NUMBER_OF_SAMPLES = 1399\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1500it [12:30,  1.63it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7273122061214577, var=0.13930728333936773\n","                Prec@(1) = 0.6290860573715811\n","                Prec@(3) = 0.8012008005336891\n","                Prec@(5) = 0.8539026017344896\n","                Prec@(10) = 0.895930620413609\n","                Prec@(50) = 0.9566377585056705\n","                NUMBER_OF_SAMPLES = 1499\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1600it [13:24,  2.60it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7313878083248604, var=0.13757780841499487\n","                Prec@(1) = 0.632895559724828\n","                Prec@(3) = 0.8061288305190745\n","                Prec@(5) = 0.8574108818011257\n","                Prec@(10) = 0.8986866791744841\n","                Prec@(50) = 0.9568480300187617\n","                NUMBER_OF_SAMPLES = 1599\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1701it [14:03,  3.64it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7332622077785046, var=0.13846236040586785\n","                Prec@(1) = 0.6380223660977046\n","                Prec@(3) = 0.8034137728075338\n","                Prec@(5) = 0.8552089464390819\n","                Prec@(10) = 0.8964096527369041\n","                Prec@(50) = 0.9540906415538553\n","                NUMBER_OF_SAMPLES = 1699\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1800it [14:51,  1.48it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7340036235924584, var=0.1380606571346907\n","                Prec@(1) = 0.6386881600889383\n","                Prec@(3) = 0.8037798777098388\n","                Prec@(5) = 0.8560311284046692\n","                Prec@(10) = 0.896609227348527\n","                Prec@(50) = 0.9560867148415787\n","                NUMBER_OF_SAMPLES = 1799\n","            \n"]},{"output_type":"stream","name":"stderr","text":["1900it [15:34,  2.64it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7330712905388772, var=0.13852601920689336\n","                Prec@(1) = 0.6377040547656662\n","                Prec@(3) = 0.8020010531858873\n","                Prec@(5) = 0.8530805687203792\n","                Prec@(10) = 0.8962611901000527\n","                Prec@(50) = 0.956292785676672\n","                NUMBER_OF_SAMPLES = 1899\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2000it [16:11,  2.79it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7320744715455726, var=0.13834577876986093\n","                Prec@(1) = 0.6353176588294147\n","                Prec@(3) = 0.8029014507253627\n","                Prec@(5) = 0.8529264632316158\n","                Prec@(10) = 0.8959479739869936\n","                Prec@(50) = 0.9569784892446224\n","                NUMBER_OF_SAMPLES = 1999\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2101it [16:52,  2.93it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7283306867856705, var=0.1392562010541808\n","                Prec@(1) = 0.6303001429252024\n","                Prec@(3) = 0.7984754645069081\n","                Prec@(5) = 0.851357789423535\n","                Prec@(10) = 0.8951881848499286\n","                Prec@(50) = 0.9575988565983802\n","                NUMBER_OF_SAMPLES = 2099\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2200it [17:31,  2.49it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7246800431670232, var=0.1404764282528079\n","                Prec@(1) = 0.626193724420191\n","                Prec@(3) = 0.7953615279672579\n","                Prec@(5) = 0.848112778535698\n","                Prec@(10) = 0.8926784902228285\n","                Prec@(50) = 0.956343792633015\n","                NUMBER_OF_SAMPLES = 2199\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2300it [18:10,  2.33it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7203189365537066, var=0.14111042084852912\n","                Prec@(1) = 0.6198347107438017\n","                Prec@(3) = 0.7929534580252283\n","                Prec@(5) = 0.8477598956067856\n","                Prec@(10) = 0.8925619834710744\n","                Prec@(50) = 0.9560678555893867\n","                NUMBER_OF_SAMPLES = 2299\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2401it [18:50,  2.75it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.717647177760092, var=0.14124431538180318\n","                Prec@(1) = 0.6148395164651939\n","                Prec@(3) = 0.7936640266777825\n","                Prec@(5) = 0.8470195914964569\n","                Prec@(10) = 0.8912046686119216\n","                Prec@(50) = 0.9537307211338057\n","                NUMBER_OF_SAMPLES = 2399\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2500it [19:29,  2.45it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7220643404131545, var=0.1404006902924593\n","                Prec@(1) = 0.6210484193677471\n","                Prec@(3) = 0.79671868747499\n","                Prec@(5) = 0.8495398159263705\n","                Prec@(10) = 0.8923569427771109\n","                Prec@(50) = 0.9535814325730292\n","                NUMBER_OF_SAMPLES = 2499\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2600it [20:16,  2.59it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7232805936274715, var=0.14015997061132265\n","                Prec@(1) = 0.6225471335128896\n","                Prec@(3) = 0.7983839938437861\n","                Prec@(5) = 0.850327048864948\n","                Prec@(10) = 0.8918814928818777\n","                Prec@(50) = 0.9522893420546364\n","                NUMBER_OF_SAMPLES = 2599\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2701it [20:56,  2.73it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7224512168345909, var=0.13989230430130203\n","                Prec@(1) = 0.6206002223045572\n","                Prec@(3) = 0.7980733605038903\n","                Prec@(5) = 0.8510559466469063\n","                Prec@(10) = 0.8936643201185624\n","                Prec@(50) = 0.9525750277880697\n","                NUMBER_OF_SAMPLES = 2699\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2801it [21:36,  2.93it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.723284512200846, var=0.1395525968408766\n","                Prec@(1) = 0.6216505894962486\n","                Prec@(3) = 0.7988567345480528\n","                Prec@(5) = 0.8524473026080743\n","                Prec@(10) = 0.894962486602358\n","                Prec@(50) = 0.9531975705609146\n","                NUMBER_OF_SAMPLES = 2799\n","            \n"]},{"output_type":"stream","name":"stderr","text":["2900it [22:16,  2.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7277695589471972, var=0.138023866415364\n","                Prec@(1) = 0.6267678509830976\n","                Prec@(3) = 0.8040703690927906\n","                Prec@(5) = 0.8558123490858917\n","                Prec@(10) = 0.8975508796136599\n","                Prec@(50) = 0.9541221110727838\n","                NUMBER_OF_SAMPLES = 2899\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3000it [23:03,  1.98it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7269322583737823, var=0.13798928038796876\n","                Prec@(1) = 0.6252084028009336\n","                Prec@(3) = 0.804268089363121\n","                Prec@(5) = 0.8559519839946649\n","                Prec@(10) = 0.8979659886628877\n","                Prec@(50) = 0.9539846615538513\n","                NUMBER_OF_SAMPLES = 2999\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3101it [23:50,  2.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7227207931185224, var=0.1390324738244245\n","                Prec@(1) = 0.6195546950629235\n","                Prec@(3) = 0.8005808325266215\n","                Prec@(5) = 0.8538238141335914\n","                Prec@(10) = 0.8967408841561794\n","                Prec@(50) = 0.952242658922233\n","                NUMBER_OF_SAMPLES = 3099\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3200it [24:45,  1.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7177462242327078, var=0.13976990158239178\n","                Prec@(1) = 0.6123788683963739\n","                Prec@(3) = 0.795873710534542\n","                Prec@(5) = 0.8540168802750859\n","                Prec@(10) = 0.8968427633635511\n","                Prec@(50) = 0.9524851516098781\n","                NUMBER_OF_SAMPLES = 3199\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3300it [25:34,  2.00it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7191055787126427, var=0.13951375786250292\n","                Prec@(1) = 0.6141254925735071\n","                Prec@(3) = 0.7975143983025159\n","                Prec@(5) = 0.854501364049712\n","                Prec@(10) = 0.8966353440436496\n","                Prec@(50) = 0.9518035768414671\n","                NUMBER_OF_SAMPLES = 3299\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3400it [26:29,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7187689758443021, var=0.13970566024557987\n","                Prec@(1) = 0.6137099146807885\n","                Prec@(3) = 0.7969991173874669\n","                Prec@(5) = 0.8531921153280376\n","                Prec@(10) = 0.8955575169167402\n","                Prec@(50) = 0.9514563106796117\n","                NUMBER_OF_SAMPLES = 3399\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3501it [27:12,  2.79it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7148892586937194, var=0.14063257461076012\n","                Prec@(1) = 0.6084595598742498\n","                Prec@(3) = 0.7945127179194056\n","                Prec@(5) = 0.8499571306087453\n","                Prec@(10) = 0.8933981137467848\n","                Prec@(50) = 0.9505573020863104\n","                NUMBER_OF_SAMPLES = 3499\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3600it [27:56,  2.12it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7124572679806125, var=0.1404772117604505\n","                Prec@(1) = 0.60377882745207\n","                Prec@(3) = 0.7949430397332592\n","                Prec@(5) = 0.8505140316754654\n","                Prec@(10) = 0.8938594053903862\n","                Prec@(50) = 0.9505418171714365\n","                NUMBER_OF_SAMPLES = 3599\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3700it [28:38,  2.34it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7104183314076932, var=0.14120532151596038\n","                Prec@(1) = 0.6017842660178426\n","                Prec@(3) = 0.7923763179237632\n","                Prec@(5) = 0.8486077318194106\n","                Prec@(10) = 0.8932143822654771\n","                Prec@(50) = 0.9510678561773452\n","                NUMBER_OF_SAMPLES = 3699\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3800it [29:22,  1.61it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7118527162483748, var=0.14108700928824341\n","                Prec@(1) = 0.6041063437746775\n","                Prec@(3) = 0.7933666754409056\n","                Prec@(5) = 0.8486443801000263\n","                Prec@(10) = 0.8933929981574098\n","                Prec@(50) = 0.951302974466965\n","                NUMBER_OF_SAMPLES = 3799\n","            \n"]},{"output_type":"stream","name":"stderr","text":["3900it [30:18,  3.39it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7068083290028647, var=0.142900459786548\n","                Prec@(1) = 0.5988715055142344\n","                Prec@(3) = 0.7873813798409849\n","                Prec@(5) = 0.8435496281097717\n","                Prec@(10) = 0.8897153116183637\n","                Prec@(50) = 0.951013080276994\n","                NUMBER_OF_SAMPLES = 3899\n","            \n"]},{"output_type":"stream","name":"stderr","text":["4000it [31:12,  1.89it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7087228867912012, var=0.14229687392919907\n","                Prec@(1) = 0.6009002250562641\n","                Prec@(3) = 0.7891972993248312\n","                Prec@(5) = 0.8452113028257064\n","                Prec@(10) = 0.8904726181545386\n","                Prec@(50) = 0.9509877469367342\n","                NUMBER_OF_SAMPLES = 3999\n","            \n"]},{"output_type":"stream","name":"stderr","text":["4100it [32:14,  2.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7078666251498227, var=0.14220498192120987\n","                Prec@(1) = 0.599170529397414\n","                Prec@(3) = 0.7892168821663821\n","                Prec@(5) = 0.8455720907538424\n","                Prec@(10) = 0.8907050500121981\n","                Prec@(50) = 0.9509636496706514\n","                NUMBER_OF_SAMPLES = 4099\n","            \n"]},{"output_type":"stream","name":"stderr","text":["4201it [32:58,  2.68it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7083708545360687, var=0.1422090041168014\n","                Prec@(1) = 0.600142891164563\n","                Prec@(3) = 0.7894736842105263\n","                Prec@(5) = 0.8452012383900929\n","                Prec@(10) = 0.8911645629911884\n","                Prec@(50) = 0.9511788521076446\n","                NUMBER_OF_SAMPLES = 4199\n","            \n"]},{"output_type":"stream","name":"stderr","text":["4300it [33:52,  2.19it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","                MRR: mean=0.7090700783203909, var=0.1425318346411204\n","                Prec@(1) = 0.6020004652244708\n","                Prec@(3) = 0.7887880902535473\n","                Prec@(5) = 0.8436845778087927\n","                Prec@(10) = 0.8909048615957199\n","                Prec@(50) = 0.9506862060944405\n","                NUMBER_OF_SAMPLES = 4299\n","            \n"]},{"output_type":"stream","name":"stderr","text":["4340it [34:07,  2.12it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-0083e62004a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_FUDNet_DR_TEIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-0a1c1deada4e>\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(df, predictor)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mactual_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_doc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-249f72cb4912>\u001b[0m in \u001b[0;36mpredict_FUDNet_DR_TEIT\u001b[0;34m(data, k)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_followup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdr_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_DR_TEIT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prev_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdr_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-4e20684b4bfc>\u001b[0m in \u001b[0;36mpredict_DR_TEIT\u001b[0;34m(queries, k, alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcoef_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mquery_embd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         query_sim = list(map(lambda x: np.dot(x, query_embd) /\n\u001b[1;32m     18\u001b[0m                             \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-3eece1f5e79c>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(sentece)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                 padding=True)\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 )\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         )\n\u001b[1;32m    479\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     ):\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print(test_loop(train_df, predict_FUDNet_DR_TEIT))"]},{"cell_type":"code","source":["print(test_loop(train_df, predict_KNN_FUDNet_DR_TEIT))"],"metadata":{"id":"SdGwYLfIxV3X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eH9vqWah_hFc"},"source":["# Results\n","\n","At last we have resutls as follows:\n","\n","\n","| Method | @1 | @5 | @10 | @50 | @100 | MRR (mean, var) |\n","|:------:|:------:|:------:|:-------:|:-------:|:--------:|:---:|\n","| IDF - vanilla | 13% | 30% | 39% | 64% | 83% | (0.22, 0.11) |\n","| IDF - power-order | 15% | 31% | 41% | 65% | 83% | (0.23, 0.12) |\n","| IDF - power-order (softmax) | 10.7% | 23% | 31% | 57.6% | 78% | (0.18, 0.09) |\n","| IDF - self-attention | 13.9% | 29% | 38% | 62% | 82% | (0.22, 0.11) |\n","| DR. TEIT | 61.6% | 86% | 91% | 96% | 98% | (0.72, 0.13) |\n","| DR. TEIT (val) | 40.0% | 69.25% | 92.4% | 99% | 98% | (0.53, 0.16) |\n","| FUDNet + DR. TEIT | 67% | 87% | 91% | 96% | 99% | (0.76, 0.12) |\n","| FUDNet + DR. TEIT (val) | 48.2% | 75.48% | 82% | 93% | 99% | (0.60, 0.16) |"]},{"cell_type":"markdown","source":["MRR: mean=0.7605851824924286, var=0.12850709866222307\n","                Prec@(1) = 0.6706096451319381\n","                Prec@(5) = 0.8698817106460418\n","                Prec@(10) = 0.910828025477707\n","                Prec@(50) = 0.9681528662420382\n","                NUMBER_OF_SAMPLES = 1099"],"metadata":{"id":"SczaywmN6Y0m"}},{"cell_type":"markdown","metadata":{"id":"cJ3M9TEuFq_M"},"source":["# drafts"]},{"cell_type":"code","source":["def simple_fudnet_test():\n","  data = {\"combined\": \" <SEP> Hello. how can you help me?\", \"label\": 0}\n","  inputs = tokenize_function(data, prediction=True, cuda=True)\n","  labels = torch.tensor([data['label']]).unsqueeze(0)  # Batch size 1\n","  outputs = fudnet_model(**inputs)\n","  prediction = torch.argmax(outputs.logits)\n","  print(prediction)\n","\n","simple_test()"],"metadata":{"id":"VxPrLsnz-ccc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbPuutLoOvsK"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5EncoderModel\n","import torch\n","\n","model_name = [\"setu4993/LaBSE\", \"t5-small\", \"bert-base-uncased\"][0]\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","\n","# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","# model = T5EncoderModel.from_pretrained(\"t5-base\")\n","\n","inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","outputs = model(**inputs)\n","\n","last_hidden_states = outputs.last_hidden_state\n","print(\"inputs\", inputs)\n","\n","print(\"last_hidden_states\", last_hidden_states.shape)\n","\n","# pooler = outputs.pooler_output\n","# print(\"pooler\",pooler.shape)\n","# with torch.no_grad():\n","#     print(np.squeeze(np.array(pooler)).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZlFbxcxK3Ljs"},"outputs":[],"source":["import numpy as np\n","\n","from utils import scaled_dot_product_attention\n","\n","inputs = tokenizer([\"Hello, my dog is cute\", \"Yes this is a beautiful dog and you can have it.\"], max_length=16, padding=\"max_length\", return_tensors=\"pt\")\n","outputs = model(**inputs)\n","\n","query=last_hidden_states[0, :, :]\n","key=last_hidden_states[1, :, :]\n","value=last_hidden_states[1, :, :]\n","\n","context, attention = scaled_dot_product_attention(\n","    query=query,\n","    key=key,\n","    value=value,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQVqMhWX3Ljt","outputId":"82a25d9c-062b-4718-d282-cb16d839aa3b"},"outputs":[{"data":{"text/plain":["(torch.Size([16, 768]), torch.Size([16, 16]))"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["context.shape , attention.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGySpDFl3Lju","outputId":"9a72f319-e934-4272-962c-6b5defd4c85b"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","with torch.no_grad():\n","    plt.imshow(attention, interpolation='nearest')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0RaKNZY3Ljv","outputId":"40a57ea1-25e6-4c09-933f-03aff38a8942"},"outputs":[{"data":{"text/plain":["tensor([[4.1652e-05, 1.8743e-08, 4.3531e-15, 1.3554e-17, 2.0464e-16, 1.3552e-08,\n","         9.9992e-01, 1.8685e-20, 2.9881e-23, 3.2375e-22, 4.0948e-22, 5.8909e-23,\n","         5.1022e-18, 4.1652e-05, 1.0425e-15, 3.1295e-16],\n","        [6.9659e-16, 5.0016e-07, 1.9475e-12, 6.0816e-15, 4.3889e-14, 1.0717e-07,\n","         1.0000e+00, 1.2967e-17, 1.6856e-19, 1.3311e-18, 2.6679e-18, 2.0186e-19,\n","         6.4972e-16, 6.9659e-16, 3.3120e-14, 1.4395e-14],\n","        [2.8160e-17, 4.6376e-12, 3.3828e-09, 3.1156e-10, 7.6270e-09, 3.4791e-07,\n","         1.0000e+00, 1.6709e-13, 7.2155e-15, 7.7347e-15, 2.2479e-14, 1.4072e-14,\n","         3.9355e-12, 2.8160e-17, 7.8200e-14, 8.5553e-14],\n","        [8.9966e-22, 3.2495e-17, 1.8656e-13, 6.4120e-14, 1.5420e-11, 2.0936e-09,\n","         1.0000e+00, 3.8246e-18, 1.3870e-19, 3.8302e-19, 3.0957e-18, 2.5225e-19,\n","         1.7452e-17, 8.9966e-22, 3.4182e-18, 3.9053e-18],\n","        [1.0318e-33, 1.2762e-28, 2.2427e-30, 7.8236e-32, 1.9107e-27, 3.6965e-22,\n","         1.0000e+00, 3.1281e-36, 1.3927e-38, 2.2895e-37, 1.5183e-36, 5.2110e-38,\n","         4.2408e-35, 1.0318e-33, 5.6916e-34, 2.6557e-34],\n","        [2.9877e-19, 1.8721e-15, 7.1188e-10, 7.5957e-10, 9.5682e-08, 1.9970e-06,\n","         1.0000e+00, 8.9723e-14, 7.0886e-15, 7.2686e-15, 2.1089e-14, 9.9914e-15,\n","         2.6498e-13, 2.9877e-19, 2.0165e-15, 2.6876e-15],\n","        [7.4719e-22, 5.1469e-17, 6.6067e-16, 1.9406e-16, 2.8237e-13, 1.2238e-05,\n","         9.9999e-01, 2.6902e-20, 6.5572e-22, 3.2489e-21, 8.4031e-21, 5.6765e-22,\n","         3.0449e-20, 7.4719e-22, 4.7050e-20, 4.0429e-20],\n","        [4.1652e-05, 1.8743e-08, 4.3531e-15, 1.3554e-17, 2.0464e-16, 1.3552e-08,\n","         9.9992e-01, 1.8685e-20, 2.9880e-23, 3.2375e-22, 4.0949e-22, 5.8910e-23,\n","         5.1022e-18, 4.1652e-05, 1.0425e-15, 3.1296e-16],\n","        [5.9133e-15, 9.5168e-11, 2.9626e-09, 5.4552e-11, 8.0166e-10, 4.4084e-07,\n","         1.0000e+00, 1.6482e-12, 5.3448e-14, 5.9957e-14, 2.1881e-13, 1.3077e-13,\n","         1.2206e-11, 5.9133e-15, 5.7231e-07, 3.3679e-07],\n","        [9.3829e-15, 1.1562e-10, 2.0312e-09, 3.8886e-11, 5.3428e-10, 3.4737e-07,\n","         1.0000e+00, 1.2102e-12, 3.6069e-14, 4.4100e-14, 1.6976e-13, 9.6020e-14,\n","         9.4887e-12, 9.3829e-15, 8.3791e-07, 5.1095e-07],\n","        [5.2414e-15, 8.6201e-11, 1.4455e-09, 2.4929e-11, 3.8093e-10, 3.0065e-07,\n","         1.0000e+00, 8.3030e-13, 2.2826e-14, 2.7789e-14, 1.2366e-13, 6.0846e-14,\n","         5.6131e-12, 5.2414e-15, 1.0698e-06, 5.7978e-07],\n","        [3.0958e-15, 4.6744e-11, 1.6457e-09, 2.8861e-11, 4.2559e-10, 2.4767e-07,\n","         1.0000e+00, 8.3148e-13, 2.3863e-14, 2.9668e-14, 1.1722e-13, 6.3246e-14,\n","         5.7814e-12, 3.0958e-15, 9.5961e-07, 4.9884e-07],\n","        [1.8006e-14, 1.9729e-10, 1.4437e-09, 2.0318e-11, 2.4315e-10, 2.3768e-07,\n","         1.0000e+00, 7.8998e-13, 1.9282e-14, 2.3731e-14, 1.0645e-13, 5.7221e-14,\n","         6.2588e-12, 1.8006e-14, 2.6341e-06, 1.4341e-06],\n","        [8.4964e-15, 1.3390e-10, 1.1086e-09, 1.4253e-11, 1.9078e-10, 2.0294e-07,\n","         1.0000e+00, 6.0238e-13, 1.3977e-14, 1.6624e-14, 7.8173e-14, 3.9471e-14,\n","         4.1144e-12, 8.4965e-15, 2.1830e-06, 1.1469e-06],\n","        [2.4448e-14, 2.9755e-10, 1.9748e-09, 2.3722e-11, 2.6664e-10, 2.9164e-07,\n","         9.9999e-01, 1.1013e-12, 2.8862e-14, 3.5822e-14, 1.4386e-13, 8.3851e-14,\n","         9.6447e-12, 2.4448e-14, 4.6905e-06, 2.2014e-06],\n","        [1.8272e-14, 2.1756e-10, 1.5898e-09, 2.3002e-11, 2.6720e-10, 2.2422e-07,\n","         1.0000e+00, 9.2031e-13, 2.4502e-14, 3.0381e-14, 1.2275e-13, 7.0096e-14,\n","         7.7049e-12, 1.8272e-14, 2.8986e-06, 1.6224e-06]],\n","       grad_fn=<SoftmaxBackward0>)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9upZg7Ge3Ljw"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["kuhM4K4J1E3Y"],"provenance":[],"toc_visible":true,"machine_shape":"hm"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"84dc93c342a84942b82ebf6075aeda07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6965c6e725b34188ae3137fa7eac33a3","IPY_MODEL_0372fe2e9a22479d94d7548fb2d5501d","IPY_MODEL_a1441a04ba044667b3ac6d43d31f37c2"],"layout":"IPY_MODEL_9ab3c293d7eb4ee0abafe4aeb4effb74"}},"6965c6e725b34188ae3137fa7eac33a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4dbfc5d33af424babbbf2e75cb7f78d","placeholder":"​","style":"IPY_MODEL_99ddc0083bc4487b90f6bea792d75c07","value":"100%"}},"0372fe2e9a22479d94d7548fb2d5501d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23c843c0afa544f48e802b858ad9eaad","max":78,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a28213d2b6f544ef95b008da78e383f8","value":78}},"a1441a04ba044667b3ac6d43d31f37c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c44e38f6a1a490eb8a2446a2e52ba69","placeholder":"​","style":"IPY_MODEL_a842013d7ebb4c7187832e548ff75086","value":" 78/78 [01:09&lt;00:00,  1.26ba/s]"}},"9ab3c293d7eb4ee0abafe4aeb4effb74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4dbfc5d33af424babbbf2e75cb7f78d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99ddc0083bc4487b90f6bea792d75c07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23c843c0afa544f48e802b858ad9eaad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a28213d2b6f544ef95b008da78e383f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c44e38f6a1a490eb8a2446a2e52ba69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a842013d7ebb4c7187832e548ff75086":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d599381394c44f7bd447013033c30a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_506574c2dc5a4c21b4a982a09f7e1cfc","IPY_MODEL_6c8c301534e24e5d957aec3b83636204","IPY_MODEL_6700930c83b34ffb8e0b9aa13ac70013"],"layout":"IPY_MODEL_975f64047561442a9b63f61ee47507a6"}},"506574c2dc5a4c21b4a982a09f7e1cfc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c86a16d488ec407dacca60690e9eaf06","placeholder":"​","style":"IPY_MODEL_0f100ba0114b4366bff0b28e91f57b76","value":"100%"}},"6c8c301534e24e5d957aec3b83636204":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dfa8b12b1174f518414c84ddc3e842c","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_afde3cf3365c411c8fb2152794db2b6c","value":15}},"6700930c83b34ffb8e0b9aa13ac70013":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edcf7d01f07a43b4801894af5227f951","placeholder":"​","style":"IPY_MODEL_547b4391c4c14231964f32fbfb4efd79","value":" 15/15 [00:13&lt;00:00,  1.05s/ba]"}},"975f64047561442a9b63f61ee47507a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c86a16d488ec407dacca60690e9eaf06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f100ba0114b4366bff0b28e91f57b76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dfa8b12b1174f518414c84ddc3e842c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afde3cf3365c411c8fb2152794db2b6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"edcf7d01f07a43b4801894af5227f951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"547b4391c4c14231964f32fbfb4efd79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}